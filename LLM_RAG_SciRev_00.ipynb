{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/shakil1819/LLM-RAG-SCiRev/blob/main/LLM_RAG_SciRev_00.ipynb",
      "authorship_tag": "ABX9TyNCSN3lARf9Gy+vISu87AoQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7a548280764e451395a4eada51727a88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c02177090e84eb1a844f008aed891b5",
              "IPY_MODEL_fe1905ac15eb430eaf7523e26fdaf7df",
              "IPY_MODEL_31e7cafb682d48688766cf29690e57eb"
            ],
            "layout": "IPY_MODEL_d43619363a154cffb858dafd81436ad5"
          }
        },
        "9c02177090e84eb1a844f008aed891b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f50fcf3bd66480c90f5c74688ec62ef",
            "placeholder": "​",
            "style": "IPY_MODEL_23c3546cdc1c4c91a96abed8000b1c55",
            "value": "Downloading (…)e9125/.gitattributes: 100%"
          }
        },
        "fe1905ac15eb430eaf7523e26fdaf7df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45f3c93ef95f4e18b5dd53279d34a259",
            "max": 1175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50c6b35db6fc47569b693cb302e0f538",
            "value": 1175
          }
        },
        "31e7cafb682d48688766cf29690e57eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91101939b3e54cac9aadd6048fd33a88",
            "placeholder": "​",
            "style": "IPY_MODEL_9ffb50e132944e52ab6fc529dce37ba6",
            "value": " 1.18k/1.18k [00:00&lt;00:00, 75.1kB/s]"
          }
        },
        "d43619363a154cffb858dafd81436ad5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f50fcf3bd66480c90f5c74688ec62ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23c3546cdc1c4c91a96abed8000b1c55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45f3c93ef95f4e18b5dd53279d34a259": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50c6b35db6fc47569b693cb302e0f538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91101939b3e54cac9aadd6048fd33a88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ffb50e132944e52ab6fc529dce37ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f20829e982c4a24a364141ae54b3d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f693db637f6748ed952de43b8bf0b096",
              "IPY_MODEL_0673664975f64a8d9c9a00c76f169f43",
              "IPY_MODEL_7a4e8922948c4b40a9f33c134b4d79a9"
            ],
            "layout": "IPY_MODEL_daf045668eaa46d1a4e596e4b0775af6"
          }
        },
        "f693db637f6748ed952de43b8bf0b096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae26c46039484c0183b01243a7b057fa",
            "placeholder": "​",
            "style": "IPY_MODEL_bd216d7e420443cca6b6cc188c982c11",
            "value": "Downloading (…)_Pooling/config.json: 100%"
          }
        },
        "0673664975f64a8d9c9a00c76f169f43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42bda4aa40a04e729199434f159943ad",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae90632c262042908e1294bb50e44a8f",
            "value": 190
          }
        },
        "7a4e8922948c4b40a9f33c134b4d79a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42f0ee6fab614316a438ee4436e8e568",
            "placeholder": "​",
            "style": "IPY_MODEL_d740064bc3354ebfb04bb0cf9bb6e195",
            "value": " 190/190 [00:00&lt;00:00, 12.5kB/s]"
          }
        },
        "daf045668eaa46d1a4e596e4b0775af6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae26c46039484c0183b01243a7b057fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd216d7e420443cca6b6cc188c982c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42bda4aa40a04e729199434f159943ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae90632c262042908e1294bb50e44a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42f0ee6fab614316a438ee4436e8e568": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d740064bc3354ebfb04bb0cf9bb6e195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b29a736ab014072a4a62fa4530693b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15c0995964974ca799a30943450b6833",
              "IPY_MODEL_1e95564a70734dc7af25a5305b4d7482",
              "IPY_MODEL_fe6735d80c7448fd9d31170e2346f61d"
            ],
            "layout": "IPY_MODEL_3bc3cb6e1a204bb595d008464d658b33"
          }
        },
        "15c0995964974ca799a30943450b6833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf322aad77c64a99905045a02bc6e350",
            "placeholder": "​",
            "style": "IPY_MODEL_c4541a942fc6438ab9e51dfeaff56ba0",
            "value": "Downloading (…)7e55de9125/README.md: 100%"
          }
        },
        "1e95564a70734dc7af25a5305b4d7482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30114c6327f34df5a88b478662f898f2",
            "max": 10610,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73f967e4718b4c35a46a6da15c2ff9d4",
            "value": 10610
          }
        },
        "fe6735d80c7448fd9d31170e2346f61d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ec2f7f04752418688efacb3193867ba",
            "placeholder": "​",
            "style": "IPY_MODEL_5dc9f9c2bd5b4a1393d569930c1fe317",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 736kB/s]"
          }
        },
        "3bc3cb6e1a204bb595d008464d658b33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf322aad77c64a99905045a02bc6e350": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4541a942fc6438ab9e51dfeaff56ba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30114c6327f34df5a88b478662f898f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73f967e4718b4c35a46a6da15c2ff9d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5ec2f7f04752418688efacb3193867ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dc9f9c2bd5b4a1393d569930c1fe317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69b82b1a2e0e4e54bf231082844e5a5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24e8674aa6f94e8cbcd17b1593b8b942",
              "IPY_MODEL_1d5e696f5b4442c89e24756d6d3ecc6f",
              "IPY_MODEL_a4b9b37cfb764255ab5fde6e74446951"
            ],
            "layout": "IPY_MODEL_a57c456658d340c8ab1a1bc39718b1cd"
          }
        },
        "24e8674aa6f94e8cbcd17b1593b8b942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6722b59bdf92470ca44e83b614e07ced",
            "placeholder": "​",
            "style": "IPY_MODEL_3db0929544eb4ed4baf1ba74333d97ea",
            "value": "Downloading (…)55de9125/config.json: 100%"
          }
        },
        "1d5e696f5b4442c89e24756d6d3ecc6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51c45b526d874705bedb7b7de2857db0",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccef1f34b24e4051a38f7e12c6d90cca",
            "value": 612
          }
        },
        "a4b9b37cfb764255ab5fde6e74446951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74da29651e4d44258fb3ab6687bfae74",
            "placeholder": "​",
            "style": "IPY_MODEL_c93630edfee14609851115ad97e61397",
            "value": " 612/612 [00:00&lt;00:00, 45.9kB/s]"
          }
        },
        "a57c456658d340c8ab1a1bc39718b1cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6722b59bdf92470ca44e83b614e07ced": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3db0929544eb4ed4baf1ba74333d97ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51c45b526d874705bedb7b7de2857db0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccef1f34b24e4051a38f7e12c6d90cca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74da29651e4d44258fb3ab6687bfae74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c93630edfee14609851115ad97e61397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e51a823018042a089de8282e651e21c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b39ab82f9107495f994dbb9e8c7e463c",
              "IPY_MODEL_a469e6eedf4b48a3ab8002e2f154c051",
              "IPY_MODEL_91df94c8660d4948825ed08ece80568b"
            ],
            "layout": "IPY_MODEL_56d5786dd2444f9f92a0f326e79c2b54"
          }
        },
        "b39ab82f9107495f994dbb9e8c7e463c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98af79139a9a44fa84e3f5398ea8395d",
            "placeholder": "​",
            "style": "IPY_MODEL_b8c23eedb3fa4782996ce2a1d9b845c4",
            "value": "Downloading (…)ce_transformers.json: 100%"
          }
        },
        "a469e6eedf4b48a3ab8002e2f154c051": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f7d6c69e4484bdfa795d319ec72f34f",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4a0d42edc71440cb690b48420935734",
            "value": 116
          }
        },
        "91df94c8660d4948825ed08ece80568b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73fc05b29f144c4088a284034c0c2b08",
            "placeholder": "​",
            "style": "IPY_MODEL_f1e07e727d65439191aaca71808f10ff",
            "value": " 116/116 [00:00&lt;00:00, 8.42kB/s]"
          }
        },
        "56d5786dd2444f9f92a0f326e79c2b54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98af79139a9a44fa84e3f5398ea8395d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8c23eedb3fa4782996ce2a1d9b845c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f7d6c69e4484bdfa795d319ec72f34f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4a0d42edc71440cb690b48420935734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73fc05b29f144c4088a284034c0c2b08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1e07e727d65439191aaca71808f10ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a7f9489e6234245a55fafec48d7026a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d280d5ae7fed4d46a1a7735726b15f5b",
              "IPY_MODEL_6ddc37daa89f42fe8d03c47f057ab1c5",
              "IPY_MODEL_3ae530a352c34f318ab135a6956f7568"
            ],
            "layout": "IPY_MODEL_cc817cd560584ab19ec8cf1331823dc5"
          }
        },
        "d280d5ae7fed4d46a1a7735726b15f5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b65e576d6344e488252f287c0c768ee",
            "placeholder": "​",
            "style": "IPY_MODEL_0ec11022fee1427495d3b043dd6c0f96",
            "value": "Downloading (…)125/data_config.json: 100%"
          }
        },
        "6ddc37daa89f42fe8d03c47f057ab1c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2cd54a1525a4545accd49682f5a12b0",
            "max": 39265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9586dca09fc349948687eca9276e70f9",
            "value": 39265
          }
        },
        "3ae530a352c34f318ab135a6956f7568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b352bc4f7544d2388965888156a4998",
            "placeholder": "​",
            "style": "IPY_MODEL_9380a122141a4966907afd076fbd1b0e",
            "value": " 39.3k/39.3k [00:00&lt;00:00, 606kB/s]"
          }
        },
        "cc817cd560584ab19ec8cf1331823dc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b65e576d6344e488252f287c0c768ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ec11022fee1427495d3b043dd6c0f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2cd54a1525a4545accd49682f5a12b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9586dca09fc349948687eca9276e70f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b352bc4f7544d2388965888156a4998": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9380a122141a4966907afd076fbd1b0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8bf25ffe79f34aafbdfa2f386576c29b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3849885469b4d8ca63a5bd84efeb6aa",
              "IPY_MODEL_16e8aabf20e84042a2d6daea8614a319",
              "IPY_MODEL_f6ac549b54eb4d3ab14be4d0b3c53e46"
            ],
            "layout": "IPY_MODEL_80d76b25ddea463cb4a02baca78b7964"
          }
        },
        "d3849885469b4d8ca63a5bd84efeb6aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00487600a94f47b5977368caf3d38b3a",
            "placeholder": "​",
            "style": "IPY_MODEL_20237462196a4f51805652efa64ab1e4",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "16e8aabf20e84042a2d6daea8614a319": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dba0c3e160a40fbb0345bda384dbb91",
            "max": 90888945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b101a93a4a4340869cd959e86aa29edc",
            "value": 90888945
          }
        },
        "f6ac549b54eb4d3ab14be4d0b3c53e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9918ebff3b14159b6e9fb70bbc22fb9",
            "placeholder": "​",
            "style": "IPY_MODEL_068dd2feea5449a1b0f474c941c10a87",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 193MB/s]"
          }
        },
        "80d76b25ddea463cb4a02baca78b7964": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00487600a94f47b5977368caf3d38b3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20237462196a4f51805652efa64ab1e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dba0c3e160a40fbb0345bda384dbb91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b101a93a4a4340869cd959e86aa29edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9918ebff3b14159b6e9fb70bbc22fb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "068dd2feea5449a1b0f474c941c10a87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c46ba72136345bc8c7d8c7968efc66a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ab36788dcfc4ae0873eb079008f3242",
              "IPY_MODEL_23c3ee7c304b40cf97c923d0fea9bf48",
              "IPY_MODEL_73d8e4f371c942b58eb6515e2f7456b7"
            ],
            "layout": "IPY_MODEL_51dc8547e46a4b40b5ff8385c8dbd53a"
          }
        },
        "6ab36788dcfc4ae0873eb079008f3242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da006f17bcd447dbb25baf2beb17062b",
            "placeholder": "​",
            "style": "IPY_MODEL_3fba7a9963814f50b531e6033a5a9d3c",
            "value": "Downloading (…)nce_bert_config.json: 100%"
          }
        },
        "23c3ee7c304b40cf97c923d0fea9bf48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18dfd8e4be7b43b38c041401bf07c75b",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49e7fb6ced034570ad0535dc6bcfb460",
            "value": 53
          }
        },
        "73d8e4f371c942b58eb6515e2f7456b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c6ca3c8e4474628a17dc870a4409284",
            "placeholder": "​",
            "style": "IPY_MODEL_35ad9ff803c0482da2c74c0d55e9ac87",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.75kB/s]"
          }
        },
        "51dc8547e46a4b40b5ff8385c8dbd53a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da006f17bcd447dbb25baf2beb17062b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fba7a9963814f50b531e6033a5a9d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18dfd8e4be7b43b38c041401bf07c75b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49e7fb6ced034570ad0535dc6bcfb460": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c6ca3c8e4474628a17dc870a4409284": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35ad9ff803c0482da2c74c0d55e9ac87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0f7ca8252af4b33bfe537f8e8e5635b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_558340845f22464eb0c477852e56324b",
              "IPY_MODEL_7a0ce6e4c2fe4df5bb1e9dbc38cd7d03",
              "IPY_MODEL_6815bfc7ac7347fd90914185bea3c7fb"
            ],
            "layout": "IPY_MODEL_9adbb48984fc4d8fb8cbaf8c148e7c15"
          }
        },
        "558340845f22464eb0c477852e56324b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe951f1e6b394186b194c3165ec08144",
            "placeholder": "​",
            "style": "IPY_MODEL_f3da64cab502401a8e3ffd4601e5472c",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "7a0ce6e4c2fe4df5bb1e9dbc38cd7d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55860e7dcb07412f934a4ae73fa0ecfd",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dca8f07d4e524dba8490b535749c0917",
            "value": 112
          }
        },
        "6815bfc7ac7347fd90914185bea3c7fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84c218909a9a4eaa8efbc74064761202",
            "placeholder": "​",
            "style": "IPY_MODEL_f9f0e8666df54d10963b8d6cd401d441",
            "value": " 112/112 [00:00&lt;00:00, 7.16kB/s]"
          }
        },
        "9adbb48984fc4d8fb8cbaf8c148e7c15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe951f1e6b394186b194c3165ec08144": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3da64cab502401a8e3ffd4601e5472c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55860e7dcb07412f934a4ae73fa0ecfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dca8f07d4e524dba8490b535749c0917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84c218909a9a4eaa8efbc74064761202": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9f0e8666df54d10963b8d6cd401d441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89273f39f0194b2ba451ac8496fe1f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df2a6a2cc8cc42ebb7cc30c5fc17fc01",
              "IPY_MODEL_02ec62de6ae1481cb52073f9866fe7c6",
              "IPY_MODEL_1d4eb979a3534dbeb07de1e2878ec419"
            ],
            "layout": "IPY_MODEL_f3acb2f3915a4614a76769a387e24798"
          }
        },
        "df2a6a2cc8cc42ebb7cc30c5fc17fc01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d756dd74486a4326a161501eac495259",
            "placeholder": "​",
            "style": "IPY_MODEL_dc7205912fdf473ea532d5288ab18221",
            "value": "Downloading (…)e9125/tokenizer.json: 100%"
          }
        },
        "02ec62de6ae1481cb52073f9866fe7c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a6d7a234e914c3eb6fe46cfa465ade3",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a63800d89d24b15aef336ae4ffb0514",
            "value": 466247
          }
        },
        "1d4eb979a3534dbeb07de1e2878ec419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fec1956c67b14862ac434cc5d8e31a62",
            "placeholder": "​",
            "style": "IPY_MODEL_7f779a66880e4883b34c98ec2e1e079a",
            "value": " 466k/466k [00:00&lt;00:00, 2.34MB/s]"
          }
        },
        "f3acb2f3915a4614a76769a387e24798": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d756dd74486a4326a161501eac495259": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc7205912fdf473ea532d5288ab18221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a6d7a234e914c3eb6fe46cfa465ade3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a63800d89d24b15aef336ae4ffb0514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fec1956c67b14862ac434cc5d8e31a62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f779a66880e4883b34c98ec2e1e079a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6f91c3f700b48989d78c12aa6692ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5e48d74489344b5a8dea5314b94f8d2",
              "IPY_MODEL_8d435cc50ebb443fa1bb648ebefefdf4",
              "IPY_MODEL_a7b3342a541045e3ae2fe2c9fb8c880e"
            ],
            "layout": "IPY_MODEL_c7ebcbc82ec34889bd9bd08b0b26ac8f"
          }
        },
        "c5e48d74489344b5a8dea5314b94f8d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4140a1f5a58428a878ac4b72bc8e1d0",
            "placeholder": "​",
            "style": "IPY_MODEL_4a6027bec64a4402806e6b2981e1e1d7",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "8d435cc50ebb443fa1bb648ebefefdf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f96fc837858f450e9b2686de8e735dc4",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_859a1ed8ab814dde99d42561bca3ad7d",
            "value": 350
          }
        },
        "a7b3342a541045e3ae2fe2c9fb8c880e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28b775094fff4ecdb9a4fed7e6450880",
            "placeholder": "​",
            "style": "IPY_MODEL_4ac800db8f0045108c9b57326bb721d0",
            "value": " 350/350 [00:00&lt;00:00, 15.1kB/s]"
          }
        },
        "c7ebcbc82ec34889bd9bd08b0b26ac8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4140a1f5a58428a878ac4b72bc8e1d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a6027bec64a4402806e6b2981e1e1d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f96fc837858f450e9b2686de8e735dc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "859a1ed8ab814dde99d42561bca3ad7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "28b775094fff4ecdb9a4fed7e6450880": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ac800db8f0045108c9b57326bb721d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6bfd34388b94d8abcd56c9c52234366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eafbb7500ca34f3f9f26412942da3d76",
              "IPY_MODEL_987e0faca77145d2a71558bd9ea5fa3a",
              "IPY_MODEL_775987ff960642578d3a948b6024bfbf"
            ],
            "layout": "IPY_MODEL_c5c842954a1044bdab7680222ef94911"
          }
        },
        "eafbb7500ca34f3f9f26412942da3d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d2d9fb3656940799c4101c17bbbe779",
            "placeholder": "​",
            "style": "IPY_MODEL_c07aef5a98f04d40935cbc2aea99ff02",
            "value": "Downloading (…)9125/train_script.py: 100%"
          }
        },
        "987e0faca77145d2a71558bd9ea5fa3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed9ef0ae146346b2ae02770ea1274c5b",
            "max": 13156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99b9bdb3589b49c18b66bcf8069acceb",
            "value": 13156
          }
        },
        "775987ff960642578d3a948b6024bfbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e13b6e0cab2245e2980643b64467aaa5",
            "placeholder": "​",
            "style": "IPY_MODEL_44ff0738e3164ca3a089015359bed8d5",
            "value": " 13.2k/13.2k [00:00&lt;00:00, 833kB/s]"
          }
        },
        "c5c842954a1044bdab7680222ef94911": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d2d9fb3656940799c4101c17bbbe779": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c07aef5a98f04d40935cbc2aea99ff02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed9ef0ae146346b2ae02770ea1274c5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99b9bdb3589b49c18b66bcf8069acceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e13b6e0cab2245e2980643b64467aaa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44ff0738e3164ca3a089015359bed8d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cb6f38292e64b31a813b9b407c7d565": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c679f0ca0ad4cc29fae12e18cb73a20",
              "IPY_MODEL_5b9fff8473934ee0b21f0391a44d9ab8",
              "IPY_MODEL_4105ac7f47bf4dcab7725e1e101aa89f"
            ],
            "layout": "IPY_MODEL_322143288b6345958dc145519ab3c8eb"
          }
        },
        "6c679f0ca0ad4cc29fae12e18cb73a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32af7e0017c348258954bbf2ac970c75",
            "placeholder": "​",
            "style": "IPY_MODEL_229e224195a84e5d85459a38960ff74e",
            "value": "Downloading (…)7e55de9125/vocab.txt: 100%"
          }
        },
        "5b9fff8473934ee0b21f0391a44d9ab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df766d0b355e4828a7b2d46d8993cf1d",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb183c3c95f24325bda0ac8953d2c379",
            "value": 231508
          }
        },
        "4105ac7f47bf4dcab7725e1e101aa89f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00222ec14d314f32b60780b25f3c990a",
            "placeholder": "​",
            "style": "IPY_MODEL_c0c2742de5c74a8e86a4702cc942f512",
            "value": " 232k/232k [00:00&lt;00:00, 1.76MB/s]"
          }
        },
        "322143288b6345958dc145519ab3c8eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32af7e0017c348258954bbf2ac970c75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "229e224195a84e5d85459a38960ff74e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df766d0b355e4828a7b2d46d8993cf1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb183c3c95f24325bda0ac8953d2c379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00222ec14d314f32b60780b25f3c990a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c2742de5c74a8e86a4702cc942f512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6b8bd47e1274528916720a977117d15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dcd2241e61dd4d28a25a8370d2872854",
              "IPY_MODEL_f38253e3d6b240e0beb6d39e0dee907f",
              "IPY_MODEL_d101f15a1e7f4372bb77dd4ecea158ae"
            ],
            "layout": "IPY_MODEL_b022e02b6211499a9c8e768ea297a2ed"
          }
        },
        "dcd2241e61dd4d28a25a8370d2872854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37e3f74996e14330b663885ffbb17c2b",
            "placeholder": "​",
            "style": "IPY_MODEL_ed818cfc066e44d488a835fbb51d6d6c",
            "value": "Downloading (…)5de9125/modules.json: 100%"
          }
        },
        "f38253e3d6b240e0beb6d39e0dee907f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53485f79b97640e8b981ffa5f492f6e8",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8669c23b9fb14848a39eff02287a127c",
            "value": 349
          }
        },
        "d101f15a1e7f4372bb77dd4ecea158ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24bd38e5af7d49b0ad96d3def5de9238",
            "placeholder": "​",
            "style": "IPY_MODEL_ccce57a07cc94c36aaae8b4ace864454",
            "value": " 349/349 [00:00&lt;00:00, 23.5kB/s]"
          }
        },
        "b022e02b6211499a9c8e768ea297a2ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37e3f74996e14330b663885ffbb17c2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed818cfc066e44d488a835fbb51d6d6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53485f79b97640e8b981ffa5f492f6e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8669c23b9fb14848a39eff02287a127c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24bd38e5af7d49b0ad96d3def5de9238": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccce57a07cc94c36aaae8b4ace864454": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shakil1819/LLM-RAG-SCiRev/blob/main/LLM_RAG_SciRev_00.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHe5xhawMPQV",
        "outputId": "bbbc1149-3315-4220-b63c-7d2dcc239257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.2 [186 kB]\n",
            "Fetched 186 kB in 0s (409 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 120874 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq langchain==0.0.173 --progress-bar off\n",
        "!pip install -qqq chromadb==0.3.23 --progress-bar off\n",
        "!pip install -qqq pypdf==3.8.1 --progress-bar off\n",
        "!pip install -qqq pygpt4all==1.1.0 --progress-bar off\n",
        "!pip install -qqq pdf2image==1.16.3 --progress-bar off\n",
        "!pip install llama-index\n",
        "!pip install llama-cpp-python\n",
        "!pip install langchain openai faiss-cpu tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXD2LvZGMgyg",
        "outputId": "d9c52878-6be0-4ebd-dae5-a4361f04241d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pygpt4all (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting llama-index\n",
            "  Downloading llama_index-0.8.53.post3-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (2.0.22)\n",
            "Collecting aiostream<0.6.0,>=0.5.2 (from llama-index)\n",
            "  Downloading aiostream-0.5.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.5.14)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2023.6.0)\n",
            "Collecting langchain>=0.0.303 (from llama-index)\n",
            "  Downloading langchain-0.0.324-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.8)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.23.5)\n",
            "Collecting openai>=0.26.4 (from llama-index)\n",
            "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index)\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.8.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.0)\n",
            "Collecting urllib3<2 (from llama-index)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->llama-index) (3.20.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index) (1.14.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (6.0.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (4.0.3)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain>=0.0.303->llama-index)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.52 (from langchain>=0.0.303->llama-index)\n",
            "  Downloading langsmith-0.0.52-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (2.31.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (4.66.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (1.1.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain>=0.0.303->llama-index)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->llama-index) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.303->llama-index) (2023.7.22)\n",
            "Downloading llama_index-0.8.53.post3-py3-none-any.whl (794 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.6/794.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiostream-0.5.2-py3-none-any.whl (39 kB)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading langchain-0.0.324-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.0.52-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Installing collected packages: urllib3, jsonpointer, deprecated, aiostream, jsonpatch, tiktoken, openai, langsmith, langchain, llama-index\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.0.173\n",
            "    Uninstalling langchain-0.0.173:\n",
            "      Successfully uninstalled langchain-0.0.173\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiostream-0.5.2 deprecated-1.2.14 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.324 langsmith-0.0.52 llama-index-0.8.53.post3 openai-0.28.1 tiktoken-0.5.1 urllib3-1.26.18\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.11.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.8.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.11-cp310-cp310-manylinux_2_35_x86_64.whl size=1023481 sha256=d9f85dcf3df1837d2b603fd73fbadb007b9754ddcf199cc97b11b28d8c7864d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/42/77/a3ab0d02700427ea364de5797786c0272779dce795f62c3bc2\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.11\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.324)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.14)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.52)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llmhub\n",
        "!pip install gpt4all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mogEa7T_M8Fk",
        "outputId": "a1d274a7-381a-45f3-b3ae-42b1f21da6a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llmhub\n",
            "  Downloading llmhub-0.2.3-py3-none-any.whl (3.8 kB)\n",
            "Collecting requests==2.28.1 (from llmhub)\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting auth0-python==3.24.0 (from llmhub)\n",
            "  Downloading auth0_python-3.24.0-py2.py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.6/126.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typer==0.7.0 (from llmhub)\n",
            "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyjwt>=1.7.1 in /usr/lib/python3/dist-packages (from pyjwt[crypto]>=1.7.1->auth0-python==3.24.0->llmhub) (2.3.0)\n",
            "Collecting charset-normalizer<3,>=2 (from requests==2.28.1->llmhub)\n",
            "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.1->llmhub) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.1->llmhub) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.1->llmhub) (2023.7.22)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer==0.7.0->llmhub) (8.1.7)\n",
            "Requirement already satisfied: cryptography>=3.3.1 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=1.7.1->auth0-python==3.24.0->llmhub) (41.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3.1->pyjwt[crypto]>=1.7.1->auth0-python==3.24.0->llmhub) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3.1->pyjwt[crypto]>=1.7.1->auth0-python==3.24.0->llmhub) (2.21)\n",
            "Installing collected packages: typer, charset-normalizer, requests, auth0-python, llmhub\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.0\n",
            "    Uninstalling typer-0.9.0:\n",
            "      Successfully uninstalled typer-0.9.0\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.3.1\n",
            "    Uninstalling charset-normalizer-3.3.1:\n",
            "      Successfully uninstalled charset-normalizer-3.3.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.28.1 which is incompatible.\n",
            "yfinance 0.2.31 requires requests>=2.31, but you have requests 2.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed auth0-python-3.24.0 charset-normalizer-2.1.1 llmhub-0.2.3 requests-2.28.1 typer-0.7.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting gpt4all\n",
            "  Downloading gpt4all-2.0.1-py3-none-manylinux1_x86_64.whl.metadata (892 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt4all) (2.28.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2023.7.22)\n",
            "Downloading gpt4all-2.0.1-py3-none-manylinux1_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gpt4all\n",
            "Successfully installed gpt4all-2.0.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install PyMuPDF==1.16.14\n",
        "!pip install PyPDF2\n"
      ],
      "metadata": {
        "id": "YcPvQKfWyUH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc78e13b-ce09-4ccf-8c72-7a61238f6155"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Creating a PDF reader object\n",
        "pdf_file = '/content/drive/MyDrive/Colab Notebooks/~Pictures/2023279689.pdf'\n",
        "pdf_reader = PdfReader(open(pdf_file, 'rb'))\n",
        "\n",
        "# Printing the number of pages in the PDF file\n",
        "num_pages = len(pdf_reader.pages)  # Fixed the variable name\n",
        "print(f'Total number of pages: {num_pages}')\n",
        "\n",
        "# Loop through all pages and extract text\n",
        "for page_num in range(num_pages):\n",
        "    page = pdf_reader.pages[page_num]  # Fixed the attribute access\n",
        "    text = page.extract_text()\n",
        "    print(f'Page {page_num + 1} Text:')\n",
        "    print(text)\n"
      ],
      "metadata": {
        "id": "PPS6kp65yZ69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3327f911-0fa7-494a-cdd2-3f39a0c92660"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of pages: 6\n",
            "Page 1 Text:\n",
            "CycleGAN-Based Data Augmentation with CNN\n",
            "and Vision Transformers (ViT) Models for\n",
            "Improved Maize Leaf Disease Classification\n",
            "Syed Taha Yeasin Ramadan\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "tahayeasin11@gmail.comTanjim Sakib\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "tsakib77@gmail.com\n",
            "Md. Ahsan Rahat\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "ahsanrahat11@gmail.comShakil Mosharrof\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "shakilmrf8@gmail.com\n",
            "Abstract —Crop losses pose a serious danger to global food\n",
            "security, and this problem also affects maize crops. To success-\n",
            "fully address this issue, precise disease detection techniques\n",
            "are required. However, a major hurdle to developing reliable\n",
            "models to address this issue is the dearth of datasets. In re-\n",
            "sponse, we present a novel approach that uses synthetic images\n",
            "created by CycleGAN to supplement constrained datasets. We\n",
            "thoroughly assessed deep learning models, such as ResNet50V2,\n",
            "DenseNet169, VGG16, VGG19, Xception, MobileNetV2, and\n",
            "emerging vision transformer models, such as ViT-B/16 and\n",
            "ViT-B/32, with a focus on the two critical classes of maize leaf\n",
            "diseases, blight and common rust. Notably, DenseNet169 per-\n",
            "formed better than other models with an accuracy of 98.48%,\n",
            "especially when trained on the CycleGAN-enhanced dataset.\n",
            "CycleGAN-augmented data outperformed the performance of\n",
            "the models trained solely on the original dataset, demonstrating\n",
            "the effectiveness of the augmentation approach in performance\n",
            "enhancement. By utilizing CycleGAN’s synthetic images, this\n",
            "study expands the field of maize leaf disease diagnosis and\n",
            "establishes DenseNet169 as a viable model for precise disease\n",
            "identification. The findings of the study have the potential\n",
            "to significantly revolutionize agricultural operations using ad-\n",
            "vanced maize disease detection techniques.\n",
            "Keywords —maize crops ,disease detection techniques ,syn-\n",
            "thetic images ,augmentation ,CycleGAN, deep learning models ,\n",
            "vision transformers ,accuracy .\n",
            "I. I NTRODUCTION\n",
            "Maize, scientifically known as Zea mays, is of critical\n",
            "importance in agriculture due to its numerous contributions\n",
            "to global food security, economic prosperity, and human\n",
            "well-being. Maize, being one of the world’s most frequently\n",
            "farmed crops, is a major food source for billions of people\n",
            "worldwide, particularly in Africa, Latin America, and Asia.\n",
            "Because of its adaptability, it is used to feed cattle in addition\n",
            "to humans, making it an important part of animal husbandry\n",
            "and the livestock business. In terms of annual production,\n",
            "maize is consistently one of the world’s highest-yieldingcrops. According to the Food and Agriculture Organization\n",
            "(FAO), global maize production has just surpassed 1.1\n",
            "billion metric tons, making it one of the primary contributors\n",
            "to global grain production [1]. This massive production\n",
            "reflects the crop’s tolerance to a wide range of climates\n",
            "and growing circumstances, making it a staple crop in both\n",
            "industrialized and developing countries. Notably, regions\n",
            "such as the United States, China, Brazil, and Argentina\n",
            "stand out as big maize producers, accounting for a sizable\n",
            "proportion of overall annual output.\n",
            "The worldwide maize production environment is not with-\n",
            "out difficulties, with one of the most significant setbacks\n",
            "being the impact of diseases on crop output and quality.\n",
            "Maize plants are susceptible to a variety of diseases, which\n",
            "can severely limit output levels, resulting in large economic\n",
            "losses for farmers and a negative impact on food security.\n",
            "Fungi, bacteria, and viruses, among others, contribute to\n",
            "these losses by creating diseases that impede plant growth,\n",
            "development, and overall production. The fungal disease\n",
            "”Maize Lethal Necrosis” (MLN), for example, has received\n",
            "attention due to its destructive impact on maize output. The\n",
            "virus that causes MLN is a mix of two viruses: Maize\n",
            "chlorotic mottle virus and Sugarcane mosaic virus. This\n",
            "disease spreads quickly through infected planting materials\n",
            "and insect vectors, causing severe stunting, leaf yellowing,\n",
            "and necrosis. According to studies conducted by the In-\n",
            "ternational Maize and Wheat Improvement Center (CIM-\n",
            "MYT), MLN can cause yield losses ranging from 30%\n",
            "to 100%, hurting maize production in East and Central\n",
            "Africa severely [2]. Another well-known disease is ”Maize\n",
            "Rust,” which is caused by the fungus Puccinia sorghi. This\n",
            "disease damages maize leaves, causing rust-colored lesions\n",
            "that limit photosynthesis and impede nutrient absorption.\n",
            "979-8-3503-7029-4/23/$31.00 ©2023 IEEE\n",
            "Page 2 Text:\n",
            "Maize rust outbreaks have caused output losses of up to 50%\n",
            "in regions such as Asia, Africa, and the Americas, according\n",
            "to researchers [3].\n",
            "Historically, plant disease detection methods relied on\n",
            "visual inspection by experienced professionals to identify\n",
            "signs and symptoms of plant diseases. This procedure entails\n",
            "manually inspecting plant leaves, stems, fruits, and other\n",
            "parts for distinctive patterns, discolorations, lesions, defor-\n",
            "mities, and other signs of disease presence. This procedure,\n",
            "however, is time-consuming, labor-intensive, and frequently\n",
            "prone to human mistakes. Furthermore, the knowledge re-\n",
            "quired to correctly identify a wide range of plant diseases\n",
            "may be confined to experienced specialists. The landscape\n",
            "of plant disease detection has changed due to technological\n",
            "breakthroughs and the rise of machine learning. Machine\n",
            "learning approaches have transformed the accuracy, speed,\n",
            "and scalability of disease detection systems. These methods\n",
            "make use of algorithms and computational models to assess\n",
            "enormous quantities of plant images and other pertinent data,\n",
            "allowing them to generalize and diagnose diseases across\n",
            "plant species and growth phases.\n",
            "GANs (Generative Adversarial Networks) have emerged\n",
            "as an advanced technology with impressive potential in\n",
            "disease identification. GANs have been used to improve\n",
            "disease detection systems in a variety of fields, including\n",
            "the medical and agricultural sectors, in recent years. The\n",
            "application of Vision Transformers (ViTs) in plant disease\n",
            "detection has resulted in significant advances in computer\n",
            "vision and agricultural technology. ViTs are a sort of deep\n",
            "learning architecture that was originally intended for image\n",
            "classification tasks, and their application to plant disease\n",
            "detection demonstrates their adaptability and usefulness in\n",
            "handling complicated real-world problems. Convolutional\n",
            "Neural Networks (CNNs) have traditionally been the go-\n",
            "to solution for image-based tasks such as illness diagnosis.\n",
            "ViTs, on the other hand, have emerged as a promising\n",
            "alternative that challenges the traditional paradigm. Unlike\n",
            "CNNs, which use convolutional layers to extract local\n",
            "features, ViTs take a different approach, transforming 2D\n",
            "picture data into sequences of flattened patches that are\n",
            "subsequently processed using a succession of self-attention\n",
            "methods.\n",
            "This study’s novelty is as follows:\n",
            "1) This study explores the application of GAN, in par-\n",
            "ticular CycleGAN, to address the challenges posed\n",
            "by a limited dataset in maize leaf disease analysis\n",
            "and it includes synthetic image generation and dataset\n",
            "augmentation.\n",
            "2) The evaluation phase encompasses the utilization of\n",
            "CNN and ViT models to assess performance. A com-\n",
            "parison of datasets with and without CycleGAN aug-\n",
            "mentation is conducted in this analysis. The objective\n",
            "of this method is to pinpoint superior models and to\n",
            "measure the influence of GAN’s performance.\n",
            "3) The assessment’s thoroughness is demonstrated by the\n",
            "wide range of metrics it takes into account, including\n",
            "accuracy, loss, precision, recall, and f1-score. This all-encompassing strategy guarantees a thorough compre-\n",
            "hension and understanding of the significance served\n",
            "by GAN and model performance.\n",
            "II. L ITERATURE REVIEW\n",
            "Recent advancements in the field of maize leaf disease\n",
            "detection have witnessed the integration of machine learn-\n",
            "ing techniques, particularly Convolutional Neural Networks\n",
            "(CNNs), to enhance accuracy and efficiency. Ramar Ahila\n",
            "Priyadharshini et al. [4] presented a deep convolutional\n",
            "neural network (CNN)-based architecture for maize leaf dis-\n",
            "ease classification (modified LeNet). The suggested CNNs\n",
            "were trained using maize leaf pictures from the PlantVil-\n",
            "lage dataset, with the goal of identifying four separate\n",
            "classes, three illnesses and one healthy class. In diagnosing\n",
            "maize leaf diseases, the learned model had a remarkable\n",
            "accuracy of 97.89%. Abdul Waheed et al. [5]developed\n",
            "a dense convolutional neural network (CNN) architecture\n",
            "(DenseNet) for detecting and classifying corn leaf diseases.\n",
            "The study addresses the problem of recognizing early-\n",
            "stage leaf diseases in maize crops, with the goal of im-\n",
            "proving crop health monitoring and output quality. The\n",
            "suggested optimized DenseNet model outperformed existing\n",
            "CNN designs such as EfficientNet, VGG19Net, NASNet,\n",
            "and Xception Net while requiring fewer parameters and\n",
            "computation time. Kshyanaprava Panda Panigrahi et al. [6]\n",
            "introduced a CNN-based method for detecting maize leaf\n",
            "diseases, emphasizing the importance of precise and early\n",
            "disease detection in enabling digital agricultural systems.\n",
            "To improve detection accuracy, the suggested CNN model\n",
            "included corrected linear unit activation functions, an Adam\n",
            "optimizer, and parameter tweaks. The model demonstrated\n",
            "quicker training convergence durations while achieving an\n",
            "exceptional average detection accuracy of 98.78% for three\n",
            "major maize leaf diseases, including Cercospora leaf spot,\n",
            "common rust, and northern leaf blight. IHAI ZHANG et\n",
            "al. [7] presented an improved method for automatically\n",
            "detecting maize leaf diseases using deep convolutional\n",
            "neural networks. The study offered improved GoogLeNet\n",
            "and Cifar10 models for leaf disease recognition, with an\n",
            "emphasis on parameter modifications, pooling combinations,\n",
            "dropout operations, and the use of rectified linear unit\n",
            "(Relu) functions. For recognizing eight forms of maize leaf\n",
            "diseases, the GoogLeNet model scored an amazing top-1\n",
            "average identification accuracy of 98.9%, while the Cifar10\n",
            "model achieved an average accuracy of 98.8%.\n",
            "Sumita Mishra et al. [8] used a deep convolutional neural\n",
            "network (CNN) to construct a real-time maize plant disease\n",
            "recognition system. The project aimed to reduce crop losses\n",
            "caused by corn diseases in India by employing smart devices\n",
            "for autonomous disease detection. The deep CNN model\n",
            "was adjusted by adjusting hyperparameters and pooling\n",
            "combinations. The model recognized maize leaf illnesses\n",
            "with an accuracy of 88.46%, making it suitable for real-time\n",
            "inference. Mohammad Syarief et al. [9] used convolutional\n",
            "neural networks (CNNs) to classify images of maize leaf\n",
            "disease. They used 200 photos and divided them into four\n",
            "Page 3 Text:\n",
            "categories: healthy, cercospora, common rust, and northern\n",
            "leaf blight. Seven CNN models (AlexNet, VGG16, VGG19,\n",
            "ResNet50, ResNet101, GoogleNet, and Inception-V3) and\n",
            "machine learning algorithms (SVM, kNN, Decision Tree)\n",
            "were used for feature extraction and categorization. AlexNet\n",
            "with SVM produced the best classification results, with an\n",
            "accuracy of 93.5%, sensitivity of 95.08%, and specificity\n",
            "of 93%. Dionis A. Padilla et al. [10] used a Convolutional\n",
            "Neural Network (CNN) with OpenMP implementation to\n",
            "detect corn leaf illnesses. The study used CNN and OpenMP\n",
            "to identify and classify illnesses in corn leaves. The study’s\n",
            "accuracy in detecting Leaf Blight, Leaf Rust, and Leaf Spot\n",
            "was 93%, 89%, and 89%, respectively. Yan Zhang et al.\n",
            "[11] suggested a method for detecting maize leaf disease\n",
            "using a Convolutional Neural Network (CNN) optimized\n",
            "by a Multi-Activation Function (MAF) module. The goal\n",
            "of the study was to improve the accuracy of traditional\n",
            "AI algorithms for disease identification. Due to the limited\n",
            "dataset, image pre-processing procedures were applied to\n",
            "extend and supplement the disease samples. To expedite\n",
            "training, transfer learning and warm-up procedures were\n",
            "used. The suggested MAF module enhanced the accuracy of\n",
            "mainstream CNNs, with MAF-ResNet50 achieving 97.41%\n",
            "accuracy.\n",
            "In recent years, the field of plant disease detection\n",
            "has witnessed innovative approaches that leverage Vision\n",
            "Transformers (ViTs) and Generative Adversarial Networks\n",
            "(GANs) for enhanced accuracy and robustness in disease\n",
            "identification [12]. Yan Zhang et al. [13] suggested a method\n",
            "for detecting maize leaf disease using a Convolutional Neu-\n",
            "ral Network (CNN) optimized by a Multi-Activation Func-\n",
            "tion (MAF) module. The goal of the study was to improve\n",
            "the accuracy of traditional AI algorithms for disease identi-\n",
            "fication. Due to the limited dataset, image pre-processing\n",
            "procedures were applied to extend and supplement the\n",
            "illness samples. To expedite training, transfer learning, and\n",
            "warm-up procedures were used. The suggested MAF module\n",
            "enhanced the accuracy of mainstream CNNs, with MAF-\n",
            "ResNet50 achieving 97.41% accuracy. Yasamin Borhani et\n",
            "al. [14] proposed using Vision Transformer (ViT) a deep\n",
            "learning approach for automated plant disease classification.\n",
            "Regarding the classification of maize leaf diseases using\n",
            "CNN-based models, Vision Transformer (ViT), and Cycle-\n",
            "GAN for data augmentation, we conducted a review of\n",
            "the literature and identified several gaps and research lim-\n",
            "itations. A brief overview of these challenges is provided\n",
            "below:\n",
            "1) Limited investigation of GAN approaches, notably Cy-\n",
            "cleGAN, as efficient data augmentation methods for\n",
            "overcoming the difficulties of maize leaf disease anal-\n",
            "ysis.\n",
            "2) There is a paucity of studies looking at the poten-\n",
            "tial of Vision Transformer (ViT) and Convolutional\n",
            "Neural Network (CNN) based models working with\n",
            "CycleGAN-augmented datasets to identify maize leaf\n",
            "diseases.III. M ETHODOLOGY\n",
            "The proposed methodology for detecting maize leaf dis-\n",
            "ease is based on a systematic approach that integrates\n",
            "several methodologies to improve accuracy. It all starts with\n",
            "gathering a dataset of maize leaf images [15]. This dataset\n",
            "is then pre-processed to ensure consistency and quality. The\n",
            "methodology relies on CycleGAN, a generative model, to\n",
            "alter and generate synthetic images. This synthetic image\n",
            "production step takes advantage of CycleGAN’s strengths\n",
            "to generate enhanced data with disease characteristics, in-\n",
            "creasing the dataset’s diversity and size.\n",
            "The resulting expanded dataset is then used to train\n",
            "both Convolutional Neural Network (CNN) models and\n",
            "Vision Transformers (ViTs). CNN-based models excel at\n",
            "detecting localized features and patterns, but ViTs detect\n",
            "global context inside images. The maize leaf disease de-\n",
            "tection model achieves complete image analysis, allowing\n",
            "accurate identification of diverse diseases by combining the\n",
            "capabilities of both CNNs and ViTs. In order to identify\n",
            "the optimal model, assessment metrics including accuracy,\n",
            "loss, precision, recall, and f1-score were examined both with\n",
            "and without the Cyclegan augmentation dataset. The overall\n",
            "procedure has been demonstrated in Fig. 1.\n",
            "Data Pre-\n",
            "processingCycleGANSynthetic Image\n",
            "Generation\n",
            "Augmented\n",
            "DatasetCNN and V iT\n",
            "ModelsMaize Leaf\n",
            "Disease DetectionBlast\n",
            "Common RustCollect Dataset\n",
            "Fig. 1. Overall Workflow of The Study.\n",
            "A. CycleGAN-Enhanced Synthetic Image Generation\n",
            "CycleGAN, a variant of Generative Adversarial Networks\n",
            "(GANs), plays a crucial part in enhancing images by trans-\n",
            "lating one image into another. CycleGAN, in contrast to\n",
            "conventional augmentation methods, permits the creation\n",
            "of artificial pictures in a domain without the necessity for\n",
            "paired training data. Within the CycleGAN architecture, this\n",
            "novel method uses the dynamic interaction of two generators\n",
            "and two discriminators.\n",
            "Generator A ( GA→B) transforms images from domain\n",
            "A to domain B. Given an image xAfrom domain A, the\n",
            "generator aims to produce a corresponding image xA→Bin\n",
            "domain B.\n",
            "xA→B=GA→B(xA)\n",
            "Similarly, Generator B ( GB→A) converts images from do-\n",
            "main B to domain A.\n",
            "xB→A=GB→A(xB)\n",
            "The discriminators, DAandDB, distinguish between real\n",
            "images from their respective domains and generated (fake)\n",
            "Page 4 Text:\n",
            "images. DAassesses the realism of images from domain A,\n",
            "while DBevaluates images from domain B.\n",
            "DA(xA)measures the realism of xA\n",
            "DB(xB)measures the realism of xB\n",
            "The goal of CycleGAN is to achieve image translation in\n",
            "a way that preserves the underlying content of the images.\n",
            "To accomplish this, it introduces cycle-consistency loss,\n",
            "ensuring that an image translated and then translated back\n",
            "remains similar to the original. Equation 1 is the cycle-\n",
            "consistency loss of the model.\n",
            "Lcyc(GA→B, GB→A) =ExA[∥xA−GB→A(GA→B(xA))∥1]\n",
            "(1)\n",
            "Image A\n",
            "Cyclic AGenerated\n",
            "BCompute\n",
            "Lossupdate\n",
            "updateupdate\n",
            "Image B\n",
            "Cyclic BGenerated\n",
            "ACompute\n",
            "Lossupdate\n",
            "updateupdateGenerator\n",
            "A2B\n",
            "Generator\n",
            "B2ADiscriminator\n",
            "B\n",
            "Discriminator\n",
            "AGenerator\n",
            "B2ADiscriminator\n",
            "BDiscriminator\n",
            "A\n",
            "Fake Real\n",
            "Fake RealFake RealReal Fake\n",
            "Generator\n",
            "A2B\n",
            "Fig. 2. CycleGAN architecture.\n",
            "In this instance, the capability of CycleGAN is utilized\n",
            "to convert images of healthy maize leaves into simulated\n",
            "counterparts showing the signs of blight and common rust\n",
            "maize disease images. The Generator A to B, which learns\n",
            "to change pristine leaf pictures into artificially diseased ones,\n",
            "facilitates this transformation. In contrast, Generator B to A\n",
            "produces pictures of maize leaves that appear to be in good\n",
            "condition from diseased inputs. Discriminator A compares\n",
            "the veracity of produced diseased images to real diseased\n",
            "ones, whereas Discriminator B examines the veracity of\n",
            "produced images of healthy leaves. The architecture of\n",
            "CycleGAN is illustrated in Fig. 2.\n",
            "B. Vision Transformers (ViT) model\n",
            "With the use of self-attention mechanisms and transformer\n",
            "architectures, the concept of Vision Transformers (ViT) has\n",
            "become a paradigm shift in image classification. ViT adopts\n",
            "a patch-based methodology in place of conventional Con-\n",
            "volutional Neural Networks (CNNs), turning pictures into\n",
            "sequences of patches. Each patch is hierarchically encoded\n",
            "and attentively interconnected, enabling the model to graspintricate contextual relationships and patterns across the\n",
            "image. The ”Transformer Encoder” design, which enables\n",
            "the integration of self-attention techniques across patches,\n",
            "is a critical invention of ViT. This allows the model to\n",
            "incorporate global context information, which is required for\n",
            "recognizing complex patterns and relationships in images.\n",
            "Attention (Q, K, V ) =softmax\u0012QKT\n",
            "√dk\u0013\n",
            "V (2)\n",
            "where Q,K, and Vare query, key, and value matrices,\n",
            "respectively, and dkis the dimension of the key matrix.\n",
            "ViT-B/16 improves classification accuracy by including\n",
            "wider patches, allowing for a stronger contextual knowledge\n",
            "that is necessary for differentiating between cases of blight\n",
            "and common rust maize leaf. The ”/16” stands for the patch\n",
            "size that was used to separate the input image into smaller,\n",
            "more manageable portions for processing.\n",
            "ViT-B/32 enhances classification performance even further\n",
            "by enlarging patches, allowing for greater resolution analysis\n",
            "and enhancing the difference between blight and common\n",
            "rust categories in maize leaf images. The ”/32” indicates the\n",
            "size of the provided image patch.\n",
            "Fig. 3 depicts the Vision Transformer (ViT) model’s\n",
            "architecture. ViT can capture long-range dependencies and\n",
            "contextual subtleties naturally due to this feature, which is\n",
            "essential for precise image classification.\n",
            "Linear Projection of Flattened PatchesTransformer Encoder\n",
            "1 2 8 9 7 6 5 4 3 0*MLP\n",
            "HeadClass\n",
            "Blight\n",
            "Common Rust+\n",
            "+\n",
            "Embedded\n",
            "PatchesNormMulti-Head\n",
            "AttentionNormMLPTransformer Encoder\n",
            "L ×\n",
            "Patch + Position\n",
            "Embedding\n",
            "* Extra learnable\n",
            "[class] embedding\n",
            "Fig. 3. Vision Transformers (ViT) architecture.\n",
            "C. CNN-based Model\n",
            "Convolutional neural networks, also known as CNNs,\n",
            "which architectures collect and interpret features using con-\n",
            "volutional layers, pooling layers, and fully connected layers,\n",
            "which enables them to handle complex visual patterns. For\n",
            "the purpose of detecting maize leaf disease in this study,\n",
            "a variety of deep-learning models were used. These mod-\n",
            "els include VGG16, ResNet50V2, Xception, MobileNetV2,\n",
            "DenseNet169, and VGG19. These models each provide\n",
            "distinctive architectural characteristics that enhance their\n",
            "performance in picture categorization tasks.\n",
            "DenseNet169 is known for its dense connection patterns,\n",
            "where each layer is linked to all succeeding layers, en-\n",
            "abling rich feature reuse and boosting gradient flow. On\n",
            "Page 5 Text:\n",
            "the other hand, VGG16 is a frequently used architecture\n",
            "in the area since it consists of a series of convolutional\n",
            "and pooling layers, comprising 16 layers. VGG19 adheres\n",
            "to the same architectural principles as VGG16 but has 19\n",
            "layers. ResNet50V2 uses residual connections to address\n",
            "the vanishing gradient issue and makes it possible to train\n",
            "tremendously deep networks. Xception’s architecture aims\n",
            "to capture fine-grained features through a hierarchy of trans-\n",
            "formations. MobileNetV2, which stresses efficiency through\n",
            "lightweight separable convolutions, is targeted for mobile\n",
            "and embedded devices.\n",
            "The Table I provides key factors for CNN-based and ViT\n",
            "models training, with 50 epochs and a learning rate of 0.001\n",
            "determining training duration and weight updates. By high-\n",
            "lighting robust optimization and a multi-class classification\n",
            "focus, the AdamW optimizer and Categorical Cross-entropy\n",
            "loss function improve transparency and reproducibility.\n",
            "TABLE I\n",
            "HYPERPARAMETERS FOR MODELS\n",
            "Epochs Learning Rate Optimizer Loss Function\n",
            "50 0.001 AdamW Categorical Crossentropy\n",
            "IV. E XPERIMENTS AND RESULTS\n",
            "The maize leaf disease dataset originally had 1306 sam-\n",
            "ples for common rust and 1146 samples for blight. Original\n",
            "dataset samples have been shown in Fig. 4. The dataset was\n",
            "considerably enriched through CycleGAN augmentation,\n",
            "generating 1154 blight samples and 994 common rust sam-\n",
            "ples. The dataset was adequately balanced as a consequence,\n",
            "yielding 2300 samples in total for each class. The state of\n",
            "the dataset after augmentation has been delineated in Table\n",
            "II.\n",
            "Blight\n",
            "Common\n",
            "Rust\n",
            "Fig. 4. Maize disease samples.\n",
            "Fig. 5 demonstrated the blight and common rust dis-\n",
            "ease classes’ representative samples, were produced using\n",
            "CycleGAN. These illustrations demonstrate CycleGAN’s\n",
            "capability to provide diverse and realistic disease-related\n",
            "variations.\n",
            "Blight\n",
            "Common\n",
            "RustCycleGAN Generated Synthetic Samples\n",
            "Fig. 5. CycleGAN generated synthetic samples.Table III and Fig. 7 showcase the results obtained from\n",
            "various models in the context of detecting maize leaf dis-\n",
            "eases. The evaluation is based on multiple metrics, including\n",
            "accuracy, loss, precision, recall, and F1-score.\n",
            "In the context of augmentation, the CycleGAN method\n",
            "was employed to augment the dataset, leading to improved\n",
            "model performance. Notably, the ViT-B/16 model achieved\n",
            "an accuracy of 98.37% with a loss of 0.1146, while the\n",
            "ViT-B/32 and VGG16 models attained accuracies of 97.93%\n",
            "with loss values of 0.0970 and 0.1366, respectively. The\n",
            "CycleGAN augmented ResNet50V2 and Xception models\n",
            "demonstrated accuracies of 97.61% and 97.50%, respec-\n",
            "tively, indicating their robustness in disease detection. The\n",
            "MobileNetV2 model achieved an impressive accuracy of\n",
            "98.37% with a notably low loss of 0.0684. However, the\n",
            "DenseNet169 model outperformed others with an accuracy\n",
            "of 98.48% and a significantly low loss of 0.0652, along with\n",
            "high precision, recall, and F1-score values.\n",
            "On the other hand, the table also presents results without\n",
            "augmentation. In this scenario, the models exhibited slightly\n",
            "lower accuracy values. Among these, the MobileNetV2\n",
            "model achieved an accuracy of 97.96% with a loss of 0.0798,\n",
            "while the DenseNet169 model demonstrated an accuracy\n",
            "of 97.76% and a loss of 0.0902. These results emphasize\n",
            "the importance of dataset augmentation, particularly through\n",
            "techniques like CycleGAN, in enhancing the performance of\n",
            "maize leaf disease detection models.\n",
            "TABLE II\n",
            "AUGMENTED DATASET\n",
            "Disease Initial CycleGAN Total\n",
            "Samples Augmented Samples Cumulative Samples\n",
            "Blight 1146 1154 2300\n",
            "Common Rust 1306 994 2300\n",
            "Total 2452 2148 4600\n",
            "TABLE III\n",
            "RESULT OBTAINED BYVARIOUS MODELS\n",
            "Augmentation Model Accuracy Loss Precision Recall F1-score\n",
            "Method (%)\n",
            "ViT-B/16 98.37 0.1146 0.9837 0.9837 0.9837\n",
            "ViT-B/32 97.93 0.0970 0.9794 0.9793 0.9793\n",
            "VGG16 97.93 0.1366 0.9793 0.9793 0.9793\n",
            "CycleGAN ResNet50V2 97.61 0.1574 0.9762 0.9761 0.9761\n",
            "Xception 97.50 0.1291 0.9754 0.975 0.975\n",
            "MobileNetV2 98.37 0.0684 0.9837 0.9837 0.9837\n",
            "DenseNet169 98.48 0.0652 0.9848 0.9848 0.9848\n",
            "VGG19 97.72 0.1228 0.9772 0.9772 0.9772\n",
            "ViT-B/16 96.95 0.1695 0.9695 0.9695 0.9695\n",
            "ViT-B/32 96.13 0.2120 0.9613 0.9613 0.9613\n",
            "VGG16 95.93 0.2187 0.9594 0.9593 0.9593\n",
            "Without ResNet50V2 94.91 0.2626 0.9498 0.9491 0.9491\n",
            "Augmentation Xception 95.72 0.2950 0.9572 0.9572 0.9572\n",
            "MobileNetV2 97.96 0.0798 0.9799 0.9796 0.9796\n",
            "DenseNet169 97.76 0.0902 0.9778 0.9776 0.9776\n",
            "VGG19 96.54 0.1805 0.9654 0.9654 0.9654\n",
            "Page 6 Text:\n",
            "Fig. 6. Accuracy and confusion matrix of DenseNet169 on CycleGAN-\n",
            "augmented dataset.\n",
            "0.930.940.950.960.970.980.99\n",
            "ViT-B/16\n",
            "ViT-B/32\n",
            "VGG16\n",
            "ResNet50V2\n",
            "Xception\n",
            "MobileNetV2\n",
            "DenseNet169\n",
            "VGG19\n",
            "ViT-B/16ViT-B/32\n",
            "VGG16\n",
            "ResNet50V2\n",
            "Xception\n",
            "MobileNetV2\n",
            "DenseNet169\n",
            "VGG19\n",
            "CycleGAN No Augmentation\n",
            "Precision Recall F1-score\n",
            "Fig. 7. Comparative analysis of evaluation metrics using CycleGAN\n",
            "augmented and non-augmented datasets for diverse models.\n",
            "The accuracy and confusion matrix of the best-performing\n",
            "model, DenseNet169, are illustrated in Fig. 6. The accuracy\n",
            "achieved by DenseNet169 was 98.48%, with a correspond-\n",
            "ing loss of 0.0652. The model demonstrated remarkable\n",
            "precision, recall, and F1-score values of 0.9848, 0.9848, and\n",
            "0.9848, respectively. The confusion matrix further provides\n",
            "insights into the model’s performance for each class, show-\n",
            "ing the true positive and true negative predictions as well as\n",
            "potential areas for improvement.\n",
            "V. C ONCLUSION AND FUTURE WORK\n",
            "This work demonstrated the efficacy of CycleGAN-\n",
            "generated image augmentation in improving the preciseness\n",
            "of maize leaf disease classification using various CNN-\n",
            "based and Vision Transformer (ViT) models. The findings\n",
            "underscore CycleGAN’s potential to overcome the problems\n",
            "presented by limited datasets, leading to more accurate\n",
            "disease identification. Our test results demonstrated that the\n",
            "DenseNet169 outperformed the other models, obtaining a\n",
            "maximum accuracy of 98.48% in the CycleGAN expanded\n",
            "dataset. Furthermore, the study encompasses the integration\n",
            "of a Vision Transformer (ViT) model within the scope of\n",
            "maize leaf disease detection. Future research could focus on\n",
            "adjusting CycleGAN parameters for specific disease traits\n",
            "and investigating hybrid model architectures that include\n",
            "the advantages of CNNs and ViTs for even more reliable\n",
            "classification outcomes. The study might further broaden its\n",
            "focus to include other crop diseases, which would increasethe usefulness of the suggested methodology in the field of\n",
            "managing agricultural diseases.\n",
            "REFERENCES\n",
            "[1] Food and Agriculture Organization (FAO), World Food and\n",
            "Agriculture - Statistical Pocketbook 2021 . Rome, Italy: FAO, 2021.\n",
            "[Online]. Available: http://www.fao.org/3/cb4535en/CB4535EN.pdf\n",
            "[2] I. Macharia, N. Wanyera, and P. S. Ojiambo, “Comparative assessment\n",
            "of maize lethal necrosis disease incidence and severity under varying\n",
            "agroecological conditions in kenya,” Plant Disease , vol. 101, no. 5,\n",
            "pp. 709–716, 2017.\n",
            "[3] R. P. Singh, D. P. Hodson, J. Huerta-Espino, Y . Jin, P. Njau, R. Wany-\n",
            "era, and S. Herrera-Foessel, “Will stem rust destroy the world’s wheat\n",
            "crop?” Advances in Agronomy , vol. 98, pp. 271–309, 2008.\n",
            "[4] R. Ahila Priyadharshini, S. Arivazhagan, M. Arun, and A. Mirnalini,\n",
            "“Maize leaf disease classification using deep convolutional neural\n",
            "networks,” Neural Computing and Applications , vol. 31, pp. 8887–\n",
            "8895, 2019.\n",
            "[5] A. Waheed, M. Goyal, D. Gupta, A. Khanna, A. E. Hassanien, and\n",
            "H. M. Pandey, “An optimized dense convolutional neural network\n",
            "model for disease recognition and classification in corn leaf,” Com-\n",
            "puters and Electronics in Agriculture , vol. 175, p. 105456, 2020.\n",
            "[6] K. P. Panigrahi, A. K. Sahoo, and H. Das, “A cnn approach for\n",
            "corn leaves disease detection to support digital agricultural system,”\n",
            "in2020 4th International Conference on Trends in Electronics and\n",
            "Informatics (ICOEI)(48184) . IEEE, 2020, pp. 678–683.\n",
            "[7] X. Zhang, Y . Qiao, F. Meng, C. Fan, and M. Zhang, “Identification\n",
            "of maize leaf diseases using improved deep convolutional neural\n",
            "networks,” Ieee Access , vol. 6, pp. 30 370–30 377, 2018.\n",
            "[8] S. Mishra, R. Sachan, and D. Rajpal, “Deep convolutional neural\n",
            "network based detection system for real-time corn plant disease\n",
            "recognition,” Procedia Computer Science , vol. 167, pp. 2003–2010,\n",
            "2020.\n",
            "[9] M. Syarief and W. Setiawan, “Convolutional neural network for maize\n",
            "leaf disease image classification,” Telkomnika (Telecommunication\n",
            "Computing Electronics and Control) , vol. 18, no. 3, pp. 1376–1381,\n",
            "2020.\n",
            "[10] D. A. Padilla, R. A. I. Pajes, and J. T. De Guzman, “Detection of\n",
            "corn leaf diseases using convolutional neural network with openmp\n",
            "implementation,” in 2020 IEEE 12th International Conference on\n",
            "Humanoid, Nanotechnology, Information Technology, Communication\n",
            "and Control, Environment, and Management (HNICEM) . IEEE,\n",
            "2020, pp. 1–6.\n",
            "[11] Y . Zhang, S. Wa, Y . Liu, X. Zhou, P. Sun, and Q. Ma, “High-\n",
            "accuracy detection of maize leaf diseases cnn based on multi-pathway\n",
            "activation function module,” Remote Sensing , vol. 13, no. 21, p. 4218,\n",
            "2021.\n",
            "[12] S. T. Y . Ramadan, T. Sakib, M. M. U. Haque, N. Sharmin, and\n",
            "M. M. Rahman, “Generative adversarial network-based augmented\n",
            "rice leaf disease detection using deep learning,” in 2022 25th Interna-\n",
            "tional Conference on Computer and Information Technology (ICCIT) .\n",
            "IEEE, 2022, pp. 976–981.\n",
            "[13] Y . Zhao, Z. Chen, X. Gao, W. Song, Q. Xiong, J. Hu, and Z. Zhang,\n",
            "“Plant disease detection using generated leaves based on doublegan,”\n",
            "IEEE/ACM Transactions on Computational Biology and Bioinformat-\n",
            "ics, vol. 19, no. 3, pp. 1817–1826, 2021.\n",
            "[14] Y . Borhani, J. Khoramdel, and E. Najafi, “A deep learning based\n",
            "approach for automated plant disease classification using vision\n",
            "transformer,” Scientific Reports , vol. 12, no. 1, p. 11554, 2022.\n",
            "[15] S. Ghose, “Corn or maize leaf disease dataset,”\n",
            "https://www.kaggle.com/smaranjitghose/corn-or-maize-leaf-disease-\n",
            "dataset, 2021.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import GPT4All\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from pdf2image import convert_from_path\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from operator import itemgetter\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema.runnable import RunnableMap\n",
        "from langchain.schema import format_document\n",
        "from itertools import cycle"
      ],
      "metadata": {
        "id": "mfTkdnByOGlu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://gpt4all.io/models/gguf/gpt4all-falcon-q4_0.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wPfjXOPNcvh",
        "outputId": "fbb2646f-015e-414b-f2a0-c5e8e51d7c58"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-27 11:39:14--  https://gpt4all.io/models/gguf/gpt4all-falcon-q4_0.gguf\n",
            "Resolving gpt4all.io (gpt4all.io)... 104.26.1.159, 104.26.0.159, 172.67.71.169, ...\n",
            "Connecting to gpt4all.io (gpt4all.io)|104.26.1.159|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4210419040 (3.9G)\n",
            "Saving to: ‘gpt4all-falcon-q4_0.gguf’\n",
            "\n",
            "gpt4all-falcon-q4_0 100%[===================>]   3.92G  62.9MB/s    in 69s     \n",
            "\n",
            "2023-10-27 11:40:23 (58.0 MB/s) - ‘gpt4all-falcon-q4_0.gguf’ saved [4210419040/4210419040]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images = convert_from_path(\"/content/drive/MyDrive/Colab Notebooks/~Pictures/2023279689.pdf\", dpi=88)\n",
        "len(images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78nZFsOyNzhe",
        "outputId": "18a6b24a-955f-48c1-b25e-2c5fde486ad8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"/content/drive/MyDrive/Colab Notebooks/~Pictures/2023279689.pdf\")\n",
        "documents = loader.load_and_split()"
      ],
      "metadata": {
        "id": "F9aw8n0DP9CB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7n0LYR1QHo2",
        "outputId": "5358bb92-e94d-44c5-a866-d6949d6019c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_text = \"\"\n",
        "for page in pdf_reader.pages:\n",
        "    pdf_text += page.extract_text()\n",
        "    print(pdf_text)\n"
      ],
      "metadata": {
        "id": "Xygv17GQQLHO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10686b76-ec1d-43d7-f2c1-107f12941287"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CycleGAN-Based Data Augmentation with CNN\n",
            "and Vision Transformers (ViT) Models for\n",
            "Improved Maize Leaf Disease Classification\n",
            "Syed Taha Yeasin Ramadan\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "tahayeasin11@gmail.comTanjim Sakib\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "tsakib77@gmail.com\n",
            "Md. Ahsan Rahat\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "ahsanrahat11@gmail.comShakil Mosharrof\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "shakilmrf8@gmail.com\n",
            "Abstract —Crop losses pose a serious danger to global food\n",
            "security, and this problem also affects maize crops. To success-\n",
            "fully address this issue, precise disease detection techniques\n",
            "are required. However, a major hurdle to developing reliable\n",
            "models to address this issue is the dearth of datasets. In re-\n",
            "sponse, we present a novel approach that uses synthetic images\n",
            "created by CycleGAN to supplement constrained datasets. We\n",
            "thoroughly assessed deep learning models, such as ResNet50V2,\n",
            "DenseNet169, VGG16, VGG19, Xception, MobileNetV2, and\n",
            "emerging vision transformer models, such as ViT-B/16 and\n",
            "ViT-B/32, with a focus on the two critical classes of maize leaf\n",
            "diseases, blight and common rust. Notably, DenseNet169 per-\n",
            "formed better than other models with an accuracy of 98.48%,\n",
            "especially when trained on the CycleGAN-enhanced dataset.\n",
            "CycleGAN-augmented data outperformed the performance of\n",
            "the models trained solely on the original dataset, demonstrating\n",
            "the effectiveness of the augmentation approach in performance\n",
            "enhancement. By utilizing CycleGAN’s synthetic images, this\n",
            "study expands the field of maize leaf disease diagnosis and\n",
            "establishes DenseNet169 as a viable model for precise disease\n",
            "identification. The findings of the study have the potential\n",
            "to significantly revolutionize agricultural operations using ad-\n",
            "vanced maize disease detection techniques.\n",
            "Keywords —maize crops ,disease detection techniques ,syn-\n",
            "thetic images ,augmentation ,CycleGAN, deep learning models ,\n",
            "vision transformers ,accuracy .\n",
            "I. I NTRODUCTION\n",
            "Maize, scientifically known as Zea mays, is of critical\n",
            "importance in agriculture due to its numerous contributions\n",
            "to global food security, economic prosperity, and human\n",
            "well-being. Maize, being one of the world’s most frequently\n",
            "farmed crops, is a major food source for billions of people\n",
            "worldwide, particularly in Africa, Latin America, and Asia.\n",
            "Because of its adaptability, it is used to feed cattle in addition\n",
            "to humans, making it an important part of animal husbandry\n",
            "and the livestock business. In terms of annual production,\n",
            "maize is consistently one of the world’s highest-yieldingcrops. According to the Food and Agriculture Organization\n",
            "(FAO), global maize production has just surpassed 1.1\n",
            "billion metric tons, making it one of the primary contributors\n",
            "to global grain production [1]. This massive production\n",
            "reflects the crop’s tolerance to a wide range of climates\n",
            "and growing circumstances, making it a staple crop in both\n",
            "industrialized and developing countries. Notably, regions\n",
            "such as the United States, China, Brazil, and Argentina\n",
            "stand out as big maize producers, accounting for a sizable\n",
            "proportion of overall annual output.\n",
            "The worldwide maize production environment is not with-\n",
            "out difficulties, with one of the most significant setbacks\n",
            "being the impact of diseases on crop output and quality.\n",
            "Maize plants are susceptible to a variety of diseases, which\n",
            "can severely limit output levels, resulting in large economic\n",
            "losses for farmers and a negative impact on food security.\n",
            "Fungi, bacteria, and viruses, among others, contribute to\n",
            "these losses by creating diseases that impede plant growth,\n",
            "development, and overall production. The fungal disease\n",
            "”Maize Lethal Necrosis” (MLN), for example, has received\n",
            "attention due to its destructive impact on maize output. The\n",
            "virus that causes MLN is a mix of two viruses: Maize\n",
            "chlorotic mottle virus and Sugarcane mosaic virus. This\n",
            "disease spreads quickly through infected planting materials\n",
            "and insect vectors, causing severe stunting, leaf yellowing,\n",
            "and necrosis. According to studies conducted by the In-\n",
            "ternational Maize and Wheat Improvement Center (CIM-\n",
            "MYT), MLN can cause yield losses ranging from 30%\n",
            "to 100%, hurting maize production in East and Central\n",
            "Africa severely [2]. Another well-known disease is ”Maize\n",
            "Rust,” which is caused by the fungus Puccinia sorghi. This\n",
            "disease damages maize leaves, causing rust-colored lesions\n",
            "that limit photosynthesis and impede nutrient absorption.\n",
            "979-8-3503-7029-4/23/$31.00 ©2023 IEEE\n",
            "CycleGAN-Based Data Augmentation with CNN\n",
            "and Vision Transformers (ViT) Models for\n",
            "Improved Maize Leaf Disease Classification\n",
            "Syed Taha Yeasin Ramadan\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "tahayeasin11@gmail.comTanjim Sakib\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "tsakib77@gmail.com\n",
            "Md. Ahsan Rahat\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "ahsanrahat11@gmail.comShakil Mosharrof\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "shakilmrf8@gmail.com\n",
            "Abstract —Crop losses pose a serious danger to global food\n",
            "security, and this problem also affects maize crops. To success-\n",
            "fully address this issue, precise disease detection techniques\n",
            "are required. However, a major hurdle to developing reliable\n",
            "models to address this issue is the dearth of datasets. In re-\n",
            "sponse, we present a novel approach that uses synthetic images\n",
            "created by CycleGAN to supplement constrained datasets. We\n",
            "thoroughly assessed deep learning models, such as ResNet50V2,\n",
            "DenseNet169, VGG16, VGG19, Xception, MobileNetV2, and\n",
            "emerging vision transformer models, such as ViT-B/16 and\n",
            "ViT-B/32, with a focus on the two critical classes of maize leaf\n",
            "diseases, blight and common rust. Notably, DenseNet169 per-\n",
            "formed better than other models with an accuracy of 98.48%,\n",
            "especially when trained on the CycleGAN-enhanced dataset.\n",
            "CycleGAN-augmented data outperformed the performance of\n",
            "the models trained solely on the original dataset, demonstrating\n",
            "the effectiveness of the augmentation approach in performance\n",
            "enhancement. By utilizing CycleGAN’s synthetic images, this\n",
            "study expands the field of maize leaf disease diagnosis and\n",
            "establishes DenseNet169 as a viable model for precise disease\n",
            "identification. The findings of the study have the potential\n",
            "to significantly revolutionize agricultural operations using ad-\n",
            "vanced maize disease detection techniques.\n",
            "Keywords —maize crops ,disease detection techniques ,syn-\n",
            "thetic images ,augmentation ,CycleGAN, deep learning models ,\n",
            "vision transformers ,accuracy .\n",
            "I. I NTRODUCTION\n",
            "Maize, scientifically known as Zea mays, is of critical\n",
            "importance in agriculture due to its numerous contributions\n",
            "to global food security, economic prosperity, and human\n",
            "well-being. Maize, being one of the world’s most frequently\n",
            "farmed crops, is a major food source for billions of people\n",
            "worldwide, particularly in Africa, Latin America, and Asia.\n",
            "Because of its adaptability, it is used to feed cattle in addition\n",
            "to humans, making it an important part of animal husbandry\n",
            "and the livestock business. In terms of annual production,\n",
            "maize is consistently one of the world’s highest-yieldingcrops. According to the Food and Agriculture Organization\n",
            "(FAO), global maize production has just surpassed 1.1\n",
            "billion metric tons, making it one of the primary contributors\n",
            "to global grain production [1]. This massive production\n",
            "reflects the crop’s tolerance to a wide range of climates\n",
            "and growing circumstances, making it a staple crop in both\n",
            "industrialized and developing countries. Notably, regions\n",
            "such as the United States, China, Brazil, and Argentina\n",
            "stand out as big maize producers, accounting for a sizable\n",
            "proportion of overall annual output.\n",
            "The worldwide maize production environment is not with-\n",
            "out difficulties, with one of the most significant setbacks\n",
            "being the impact of diseases on crop output and quality.\n",
            "Maize plants are susceptible to a variety of diseases, which\n",
            "can severely limit output levels, resulting in large economic\n",
            "losses for farmers and a negative impact on food security.\n",
            "Fungi, bacteria, and viruses, among others, contribute to\n",
            "these losses by creating diseases that impede plant growth,\n",
            "development, and overall production. The fungal disease\n",
            "”Maize Lethal Necrosis” (MLN), for example, has received\n",
            "attention due to its destructive impact on maize output. The\n",
            "virus that causes MLN is a mix of two viruses: Maize\n",
            "chlorotic mottle virus and Sugarcane mosaic virus. This\n",
            "disease spreads quickly through infected planting materials\n",
            "and insect vectors, causing severe stunting, leaf yellowing,\n",
            "and necrosis. According to studies conducted by the In-\n",
            "ternational Maize and Wheat Improvement Center (CIM-\n",
            "MYT), MLN can cause yield losses ranging from 30%\n",
            "to 100%, hurting maize production in East and Central\n",
            "Africa severely [2]. Another well-known disease is ”Maize\n",
            "Rust,” which is caused by the fungus Puccinia sorghi. This\n",
            "disease damages maize leaves, causing rust-colored lesions\n",
            "that limit photosynthesis and impede nutrient absorption.\n",
            "979-8-3503-7029-4/23/$31.00 ©2023 IEEEMaize rust outbreaks have caused output losses of up to 50%\n",
            "in regions such as Asia, Africa, and the Americas, according\n",
            "to researchers [3].\n",
            "Historically, plant disease detection methods relied on\n",
            "visual inspection by experienced professionals to identify\n",
            "signs and symptoms of plant diseases. This procedure entails\n",
            "manually inspecting plant leaves, stems, fruits, and other\n",
            "parts for distinctive patterns, discolorations, lesions, defor-\n",
            "mities, and other signs of disease presence. This procedure,\n",
            "however, is time-consuming, labor-intensive, and frequently\n",
            "prone to human mistakes. Furthermore, the knowledge re-\n",
            "quired to correctly identify a wide range of plant diseases\n",
            "may be confined to experienced specialists. The landscape\n",
            "of plant disease detection has changed due to technological\n",
            "breakthroughs and the rise of machine learning. Machine\n",
            "learning approaches have transformed the accuracy, speed,\n",
            "and scalability of disease detection systems. These methods\n",
            "make use of algorithms and computational models to assess\n",
            "enormous quantities of plant images and other pertinent data,\n",
            "allowing them to generalize and diagnose diseases across\n",
            "plant species and growth phases.\n",
            "GANs (Generative Adversarial Networks) have emerged\n",
            "as an advanced technology with impressive potential in\n",
            "disease identification. GANs have been used to improve\n",
            "disease detection systems in a variety of fields, including\n",
            "the medical and agricultural sectors, in recent years. The\n",
            "application of Vision Transformers (ViTs) in plant disease\n",
            "detection has resulted in significant advances in computer\n",
            "vision and agricultural technology. ViTs are a sort of deep\n",
            "learning architecture that was originally intended for image\n",
            "classification tasks, and their application to plant disease\n",
            "detection demonstrates their adaptability and usefulness in\n",
            "handling complicated real-world problems. Convolutional\n",
            "Neural Networks (CNNs) have traditionally been the go-\n",
            "to solution for image-based tasks such as illness diagnosis.\n",
            "ViTs, on the other hand, have emerged as a promising\n",
            "alternative that challenges the traditional paradigm. Unlike\n",
            "CNNs, which use convolutional layers to extract local\n",
            "features, ViTs take a different approach, transforming 2D\n",
            "picture data into sequences of flattened patches that are\n",
            "subsequently processed using a succession of self-attention\n",
            "methods.\n",
            "This study’s novelty is as follows:\n",
            "1) This study explores the application of GAN, in par-\n",
            "ticular CycleGAN, to address the challenges posed\n",
            "by a limited dataset in maize leaf disease analysis\n",
            "and it includes synthetic image generation and dataset\n",
            "augmentation.\n",
            "2) The evaluation phase encompasses the utilization of\n",
            "CNN and ViT models to assess performance. A com-\n",
            "parison of datasets with and without CycleGAN aug-\n",
            "mentation is conducted in this analysis. The objective\n",
            "of this method is to pinpoint superior models and to\n",
            "measure the influence of GAN’s performance.\n",
            "3) The assessment’s thoroughness is demonstrated by the\n",
            "wide range of metrics it takes into account, including\n",
            "accuracy, loss, precision, recall, and f1-score. This all-encompassing strategy guarantees a thorough compre-\n",
            "hension and understanding of the significance served\n",
            "by GAN and model performance.\n",
            "II. L ITERATURE REVIEW\n",
            "Recent advancements in the field of maize leaf disease\n",
            "detection have witnessed the integration of machine learn-\n",
            "ing techniques, particularly Convolutional Neural Networks\n",
            "(CNNs), to enhance accuracy and efficiency. Ramar Ahila\n",
            "Priyadharshini et al. [4] presented a deep convolutional\n",
            "neural network (CNN)-based architecture for maize leaf dis-\n",
            "ease classification (modified LeNet). The suggested CNNs\n",
            "were trained using maize leaf pictures from the PlantVil-\n",
            "lage dataset, with the goal of identifying four separate\n",
            "classes, three illnesses and one healthy class. In diagnosing\n",
            "maize leaf diseases, the learned model had a remarkable\n",
            "accuracy of 97.89%. Abdul Waheed et al. [5]developed\n",
            "a dense convolutional neural network (CNN) architecture\n",
            "(DenseNet) for detecting and classifying corn leaf diseases.\n",
            "The study addresses the problem of recognizing early-\n",
            "stage leaf diseases in maize crops, with the goal of im-\n",
            "proving crop health monitoring and output quality. The\n",
            "suggested optimized DenseNet model outperformed existing\n",
            "CNN designs such as EfficientNet, VGG19Net, NASNet,\n",
            "and Xception Net while requiring fewer parameters and\n",
            "computation time. Kshyanaprava Panda Panigrahi et al. [6]\n",
            "introduced a CNN-based method for detecting maize leaf\n",
            "diseases, emphasizing the importance of precise and early\n",
            "disease detection in enabling digital agricultural systems.\n",
            "To improve detection accuracy, the suggested CNN model\n",
            "included corrected linear unit activation functions, an Adam\n",
            "optimizer, and parameter tweaks. The model demonstrated\n",
            "quicker training convergence durations while achieving an\n",
            "exceptional average detection accuracy of 98.78% for three\n",
            "major maize leaf diseases, including Cercospora leaf spot,\n",
            "common rust, and northern leaf blight. IHAI ZHANG et\n",
            "al. [7] presented an improved method for automatically\n",
            "detecting maize leaf diseases using deep convolutional\n",
            "neural networks. The study offered improved GoogLeNet\n",
            "and Cifar10 models for leaf disease recognition, with an\n",
            "emphasis on parameter modifications, pooling combinations,\n",
            "dropout operations, and the use of rectified linear unit\n",
            "(Relu) functions. For recognizing eight forms of maize leaf\n",
            "diseases, the GoogLeNet model scored an amazing top-1\n",
            "average identification accuracy of 98.9%, while the Cifar10\n",
            "model achieved an average accuracy of 98.8%.\n",
            "Sumita Mishra et al. [8] used a deep convolutional neural\n",
            "network (CNN) to construct a real-time maize plant disease\n",
            "recognition system. The project aimed to reduce crop losses\n",
            "caused by corn diseases in India by employing smart devices\n",
            "for autonomous disease detection. The deep CNN model\n",
            "was adjusted by adjusting hyperparameters and pooling\n",
            "combinations. The model recognized maize leaf illnesses\n",
            "with an accuracy of 88.46%, making it suitable for real-time\n",
            "inference. Mohammad Syarief et al. [9] used convolutional\n",
            "neural networks (CNNs) to classify images of maize leaf\n",
            "disease. They used 200 photos and divided them into four\n",
            "CycleGAN-Based Data Augmentation with CNN\n",
            "and Vision Transformers (ViT) Models for\n",
            "Improved Maize Leaf Disease Classification\n",
            "Syed Taha Yeasin Ramadan\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "tahayeasin11@gmail.comTanjim Sakib\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "tsakib77@gmail.com\n",
            "Md. Ahsan Rahat\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "ahsanrahat11@gmail.comShakil Mosharrof\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "shakilmrf8@gmail.com\n",
            "Abstract —Crop losses pose a serious danger to global food\n",
            "security, and this problem also affects maize crops. To success-\n",
            "fully address this issue, precise disease detection techniques\n",
            "are required. However, a major hurdle to developing reliable\n",
            "models to address this issue is the dearth of datasets. In re-\n",
            "sponse, we present a novel approach that uses synthetic images\n",
            "created by CycleGAN to supplement constrained datasets. We\n",
            "thoroughly assessed deep learning models, such as ResNet50V2,\n",
            "DenseNet169, VGG16, VGG19, Xception, MobileNetV2, and\n",
            "emerging vision transformer models, such as ViT-B/16 and\n",
            "ViT-B/32, with a focus on the two critical classes of maize leaf\n",
            "diseases, blight and common rust. Notably, DenseNet169 per-\n",
            "formed better than other models with an accuracy of 98.48%,\n",
            "especially when trained on the CycleGAN-enhanced dataset.\n",
            "CycleGAN-augmented data outperformed the performance of\n",
            "the models trained solely on the original dataset, demonstrating\n",
            "the effectiveness of the augmentation approach in performance\n",
            "enhancement. By utilizing CycleGAN’s synthetic images, this\n",
            "study expands the field of maize leaf disease diagnosis and\n",
            "establishes DenseNet169 as a viable model for precise disease\n",
            "identification. The findings of the study have the potential\n",
            "to significantly revolutionize agricultural operations using ad-\n",
            "vanced maize disease detection techniques.\n",
            "Keywords —maize crops ,disease detection techniques ,syn-\n",
            "thetic images ,augmentation ,CycleGAN, deep learning models ,\n",
            "vision transformers ,accuracy .\n",
            "I. I NTRODUCTION\n",
            "Maize, scientifically known as Zea mays, is of critical\n",
            "importance in agriculture due to its numerous contributions\n",
            "to global food security, economic prosperity, and human\n",
            "well-being. Maize, being one of the world’s most frequently\n",
            "farmed crops, is a major food source for billions of people\n",
            "worldwide, particularly in Africa, Latin America, and Asia.\n",
            "Because of its adaptability, it is used to feed cattle in addition\n",
            "to humans, making it an important part of animal husbandry\n",
            "and the livestock business. In terms of annual production,\n",
            "maize is consistently one of the world’s highest-yieldingcrops. According to the Food and Agriculture Organization\n",
            "(FAO), global maize production has just surpassed 1.1\n",
            "billion metric tons, making it one of the primary contributors\n",
            "to global grain production [1]. This massive production\n",
            "reflects the crop’s tolerance to a wide range of climates\n",
            "and growing circumstances, making it a staple crop in both\n",
            "industrialized and developing countries. Notably, regions\n",
            "such as the United States, China, Brazil, and Argentina\n",
            "stand out as big maize producers, accounting for a sizable\n",
            "proportion of overall annual output.\n",
            "The worldwide maize production environment is not with-\n",
            "out difficulties, with one of the most significant setbacks\n",
            "being the impact of diseases on crop output and quality.\n",
            "Maize plants are susceptible to a variety of diseases, which\n",
            "can severely limit output levels, resulting in large economic\n",
            "losses for farmers and a negative impact on food security.\n",
            "Fungi, bacteria, and viruses, among others, contribute to\n",
            "these losses by creating diseases that impede plant growth,\n",
            "development, and overall production. The fungal disease\n",
            "”Maize Lethal Necrosis” (MLN), for example, has received\n",
            "attention due to its destructive impact on maize output. The\n",
            "virus that causes MLN is a mix of two viruses: Maize\n",
            "chlorotic mottle virus and Sugarcane mosaic virus. This\n",
            "disease spreads quickly through infected planting materials\n",
            "and insect vectors, causing severe stunting, leaf yellowing,\n",
            "and necrosis. According to studies conducted by the In-\n",
            "ternational Maize and Wheat Improvement Center (CIM-\n",
            "MYT), MLN can cause yield losses ranging from 30%\n",
            "to 100%, hurting maize production in East and Central\n",
            "Africa severely [2]. Another well-known disease is ”Maize\n",
            "Rust,” which is caused by the fungus Puccinia sorghi. This\n",
            "disease damages maize leaves, causing rust-colored lesions\n",
            "that limit photosynthesis and impede nutrient absorption.\n",
            "979-8-3503-7029-4/23/$31.00 ©2023 IEEEMaize rust outbreaks have caused output losses of up to 50%\n",
            "in regions such as Asia, Africa, and the Americas, according\n",
            "to researchers [3].\n",
            "Historically, plant disease detection methods relied on\n",
            "visual inspection by experienced professionals to identify\n",
            "signs and symptoms of plant diseases. This procedure entails\n",
            "manually inspecting plant leaves, stems, fruits, and other\n",
            "parts for distinctive patterns, discolorations, lesions, defor-\n",
            "mities, and other signs of disease presence. This procedure,\n",
            "however, is time-consuming, labor-intensive, and frequently\n",
            "prone to human mistakes. Furthermore, the knowledge re-\n",
            "quired to correctly identify a wide range of plant diseases\n",
            "may be confined to experienced specialists. The landscape\n",
            "of plant disease detection has changed due to technological\n",
            "breakthroughs and the rise of machine learning. Machine\n",
            "learning approaches have transformed the accuracy, speed,\n",
            "and scalability of disease detection systems. These methods\n",
            "make use of algorithms and computational models to assess\n",
            "enormous quantities of plant images and other pertinent data,\n",
            "allowing them to generalize and diagnose diseases across\n",
            "plant species and growth phases.\n",
            "GANs (Generative Adversarial Networks) have emerged\n",
            "as an advanced technology with impressive potential in\n",
            "disease identification. GANs have been used to improve\n",
            "disease detection systems in a variety of fields, including\n",
            "the medical and agricultural sectors, in recent years. The\n",
            "application of Vision Transformers (ViTs) in plant disease\n",
            "detection has resulted in significant advances in computer\n",
            "vision and agricultural technology. ViTs are a sort of deep\n",
            "learning architecture that was originally intended for image\n",
            "classification tasks, and their application to plant disease\n",
            "detection demonstrates their adaptability and usefulness in\n",
            "handling complicated real-world problems. Convolutional\n",
            "Neural Networks (CNNs) have traditionally been the go-\n",
            "to solution for image-based tasks such as illness diagnosis.\n",
            "ViTs, on the other hand, have emerged as a promising\n",
            "alternative that challenges the traditional paradigm. Unlike\n",
            "CNNs, which use convolutional layers to extract local\n",
            "features, ViTs take a different approach, transforming 2D\n",
            "picture data into sequences of flattened patches that are\n",
            "subsequently processed using a succession of self-attention\n",
            "methods.\n",
            "This study’s novelty is as follows:\n",
            "1) This study explores the application of GAN, in par-\n",
            "ticular CycleGAN, to address the challenges posed\n",
            "by a limited dataset in maize leaf disease analysis\n",
            "and it includes synthetic image generation and dataset\n",
            "augmentation.\n",
            "2) The evaluation phase encompasses the utilization of\n",
            "CNN and ViT models to assess performance. A com-\n",
            "parison of datasets with and without CycleGAN aug-\n",
            "mentation is conducted in this analysis. The objective\n",
            "of this method is to pinpoint superior models and to\n",
            "measure the influence of GAN’s performance.\n",
            "3) The assessment’s thoroughness is demonstrated by the\n",
            "wide range of metrics it takes into account, including\n",
            "accuracy, loss, precision, recall, and f1-score. This all-encompassing strategy guarantees a thorough compre-\n",
            "hension and understanding of the significance served\n",
            "by GAN and model performance.\n",
            "II. L ITERATURE REVIEW\n",
            "Recent advancements in the field of maize leaf disease\n",
            "detection have witnessed the integration of machine learn-\n",
            "ing techniques, particularly Convolutional Neural Networks\n",
            "(CNNs), to enhance accuracy and efficiency. Ramar Ahila\n",
            "Priyadharshini et al. [4] presented a deep convolutional\n",
            "neural network (CNN)-based architecture for maize leaf dis-\n",
            "ease classification (modified LeNet). The suggested CNNs\n",
            "were trained using maize leaf pictures from the PlantVil-\n",
            "lage dataset, with the goal of identifying four separate\n",
            "classes, three illnesses and one healthy class. In diagnosing\n",
            "maize leaf diseases, the learned model had a remarkable\n",
            "accuracy of 97.89%. Abdul Waheed et al. [5]developed\n",
            "a dense convolutional neural network (CNN) architecture\n",
            "(DenseNet) for detecting and classifying corn leaf diseases.\n",
            "The study addresses the problem of recognizing early-\n",
            "stage leaf diseases in maize crops, with the goal of im-\n",
            "proving crop health monitoring and output quality. The\n",
            "suggested optimized DenseNet model outperformed existing\n",
            "CNN designs such as EfficientNet, VGG19Net, NASNet,\n",
            "and Xception Net while requiring fewer parameters and\n",
            "computation time. Kshyanaprava Panda Panigrahi et al. [6]\n",
            "introduced a CNN-based method for detecting maize leaf\n",
            "diseases, emphasizing the importance of precise and early\n",
            "disease detection in enabling digital agricultural systems.\n",
            "To improve detection accuracy, the suggested CNN model\n",
            "included corrected linear unit activation functions, an Adam\n",
            "optimizer, and parameter tweaks. The model demonstrated\n",
            "quicker training convergence durations while achieving an\n",
            "exceptional average detection accuracy of 98.78% for three\n",
            "major maize leaf diseases, including Cercospora leaf spot,\n",
            "common rust, and northern leaf blight. IHAI ZHANG et\n",
            "al. [7] presented an improved method for automatically\n",
            "detecting maize leaf diseases using deep convolutional\n",
            "neural networks. The study offered improved GoogLeNet\n",
            "and Cifar10 models for leaf disease recognition, with an\n",
            "emphasis on parameter modifications, pooling combinations,\n",
            "dropout operations, and the use of rectified linear unit\n",
            "(Relu) functions. For recognizing eight forms of maize leaf\n",
            "diseases, the GoogLeNet model scored an amazing top-1\n",
            "average identification accuracy of 98.9%, while the Cifar10\n",
            "model achieved an average accuracy of 98.8%.\n",
            "Sumita Mishra et al. [8] used a deep convolutional neural\n",
            "network (CNN) to construct a real-time maize plant disease\n",
            "recognition system. The project aimed to reduce crop losses\n",
            "caused by corn diseases in India by employing smart devices\n",
            "for autonomous disease detection. The deep CNN model\n",
            "was adjusted by adjusting hyperparameters and pooling\n",
            "combinations. The model recognized maize leaf illnesses\n",
            "with an accuracy of 88.46%, making it suitable for real-time\n",
            "inference. Mohammad Syarief et al. [9] used convolutional\n",
            "neural networks (CNNs) to classify images of maize leaf\n",
            "disease. They used 200 photos and divided them into fourcategories: healthy, cercospora, common rust, and northern\n",
            "leaf blight. Seven CNN models (AlexNet, VGG16, VGG19,\n",
            "ResNet50, ResNet101, GoogleNet, and Inception-V3) and\n",
            "machine learning algorithms (SVM, kNN, Decision Tree)\n",
            "were used for feature extraction and categorization. AlexNet\n",
            "with SVM produced the best classification results, with an\n",
            "accuracy of 93.5%, sensitivity of 95.08%, and specificity\n",
            "of 93%. Dionis A. Padilla et al. [10] used a Convolutional\n",
            "Neural Network (CNN) with OpenMP implementation to\n",
            "detect corn leaf illnesses. The study used CNN and OpenMP\n",
            "to identify and classify illnesses in corn leaves. The study’s\n",
            "accuracy in detecting Leaf Blight, Leaf Rust, and Leaf Spot\n",
            "was 93%, 89%, and 89%, respectively. Yan Zhang et al.\n",
            "[11] suggested a method for detecting maize leaf disease\n",
            "using a Convolutional Neural Network (CNN) optimized\n",
            "by a Multi-Activation Function (MAF) module. The goal\n",
            "of the study was to improve the accuracy of traditional\n",
            "AI algorithms for disease identification. Due to the limited\n",
            "dataset, image pre-processing procedures were applied to\n",
            "extend and supplement the disease samples. To expedite\n",
            "training, transfer learning and warm-up procedures were\n",
            "used. The suggested MAF module enhanced the accuracy of\n",
            "mainstream CNNs, with MAF-ResNet50 achieving 97.41%\n",
            "accuracy.\n",
            "In recent years, the field of plant disease detection\n",
            "has witnessed innovative approaches that leverage Vision\n",
            "Transformers (ViTs) and Generative Adversarial Networks\n",
            "(GANs) for enhanced accuracy and robustness in disease\n",
            "identification [12]. Yan Zhang et al. [13] suggested a method\n",
            "for detecting maize leaf disease using a Convolutional Neu-\n",
            "ral Network (CNN) optimized by a Multi-Activation Func-\n",
            "tion (MAF) module. The goal of the study was to improve\n",
            "the accuracy of traditional AI algorithms for disease identi-\n",
            "fication. Due to the limited dataset, image pre-processing\n",
            "procedures were applied to extend and supplement the\n",
            "illness samples. To expedite training, transfer learning, and\n",
            "warm-up procedures were used. The suggested MAF module\n",
            "enhanced the accuracy of mainstream CNNs, with MAF-\n",
            "ResNet50 achieving 97.41% accuracy. Yasamin Borhani et\n",
            "al. [14] proposed using Vision Transformer (ViT) a deep\n",
            "learning approach for automated plant disease classification.\n",
            "Regarding the classification of maize leaf diseases using\n",
            "CNN-based models, Vision Transformer (ViT), and Cycle-\n",
            "GAN for data augmentation, we conducted a review of\n",
            "the literature and identified several gaps and research lim-\n",
            "itations. A brief overview of these challenges is provided\n",
            "below:\n",
            "1) Limited investigation of GAN approaches, notably Cy-\n",
            "cleGAN, as efficient data augmentation methods for\n",
            "overcoming the difficulties of maize leaf disease anal-\n",
            "ysis.\n",
            "2) There is a paucity of studies looking at the poten-\n",
            "tial of Vision Transformer (ViT) and Convolutional\n",
            "Neural Network (CNN) based models working with\n",
            "CycleGAN-augmented datasets to identify maize leaf\n",
            "diseases.III. M ETHODOLOGY\n",
            "The proposed methodology for detecting maize leaf dis-\n",
            "ease is based on a systematic approach that integrates\n",
            "several methodologies to improve accuracy. It all starts with\n",
            "gathering a dataset of maize leaf images [15]. This dataset\n",
            "is then pre-processed to ensure consistency and quality. The\n",
            "methodology relies on CycleGAN, a generative model, to\n",
            "alter and generate synthetic images. This synthetic image\n",
            "production step takes advantage of CycleGAN’s strengths\n",
            "to generate enhanced data with disease characteristics, in-\n",
            "creasing the dataset’s diversity and size.\n",
            "The resulting expanded dataset is then used to train\n",
            "both Convolutional Neural Network (CNN) models and\n",
            "Vision Transformers (ViTs). CNN-based models excel at\n",
            "detecting localized features and patterns, but ViTs detect\n",
            "global context inside images. The maize leaf disease de-\n",
            "tection model achieves complete image analysis, allowing\n",
            "accurate identification of diverse diseases by combining the\n",
            "capabilities of both CNNs and ViTs. In order to identify\n",
            "the optimal model, assessment metrics including accuracy,\n",
            "loss, precision, recall, and f1-score were examined both with\n",
            "and without the Cyclegan augmentation dataset. The overall\n",
            "procedure has been demonstrated in Fig. 1.\n",
            "Data Pre-\n",
            "processingCycleGANSynthetic Image\n",
            "Generation\n",
            "Augmented\n",
            "DatasetCNN and V iT\n",
            "ModelsMaize Leaf\n",
            "Disease DetectionBlast\n",
            "Common RustCollect Dataset\n",
            "Fig. 1. Overall Workflow of The Study.\n",
            "A. CycleGAN-Enhanced Synthetic Image Generation\n",
            "CycleGAN, a variant of Generative Adversarial Networks\n",
            "(GANs), plays a crucial part in enhancing images by trans-\n",
            "lating one image into another. CycleGAN, in contrast to\n",
            "conventional augmentation methods, permits the creation\n",
            "of artificial pictures in a domain without the necessity for\n",
            "paired training data. Within the CycleGAN architecture, this\n",
            "novel method uses the dynamic interaction of two generators\n",
            "and two discriminators.\n",
            "Generator A ( GA→B) transforms images from domain\n",
            "A to domain B. Given an image xAfrom domain A, the\n",
            "generator aims to produce a corresponding image xA→Bin\n",
            "domain B.\n",
            "xA→B=GA→B(xA)\n",
            "Similarly, Generator B ( GB→A) converts images from do-\n",
            "main B to domain A.\n",
            "xB→A=GB→A(xB)\n",
            "The discriminators, DAandDB, distinguish between real\n",
            "images from their respective domains and generated (fake)\n",
            "CycleGAN-Based Data Augmentation with CNN\n",
            "and Vision Transformers (ViT) Models for\n",
            "Improved Maize Leaf Disease Classification\n",
            "Syed Taha Yeasin Ramadan\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "tahayeasin11@gmail.comTanjim Sakib\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "tsakib77@gmail.com\n",
            "Md. Ahsan Rahat\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "ahsanrahat11@gmail.comShakil Mosharrof\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "shakilmrf8@gmail.com\n",
            "Abstract —Crop losses pose a serious danger to global food\n",
            "security, and this problem also affects maize crops. To success-\n",
            "fully address this issue, precise disease detection techniques\n",
            "are required. However, a major hurdle to developing reliable\n",
            "models to address this issue is the dearth of datasets. In re-\n",
            "sponse, we present a novel approach that uses synthetic images\n",
            "created by CycleGAN to supplement constrained datasets. We\n",
            "thoroughly assessed deep learning models, such as ResNet50V2,\n",
            "DenseNet169, VGG16, VGG19, Xception, MobileNetV2, and\n",
            "emerging vision transformer models, such as ViT-B/16 and\n",
            "ViT-B/32, with a focus on the two critical classes of maize leaf\n",
            "diseases, blight and common rust. Notably, DenseNet169 per-\n",
            "formed better than other models with an accuracy of 98.48%,\n",
            "especially when trained on the CycleGAN-enhanced dataset.\n",
            "CycleGAN-augmented data outperformed the performance of\n",
            "the models trained solely on the original dataset, demonstrating\n",
            "the effectiveness of the augmentation approach in performance\n",
            "enhancement. By utilizing CycleGAN’s synthetic images, this\n",
            "study expands the field of maize leaf disease diagnosis and\n",
            "establishes DenseNet169 as a viable model for precise disease\n",
            "identification. The findings of the study have the potential\n",
            "to significantly revolutionize agricultural operations using ad-\n",
            "vanced maize disease detection techniques.\n",
            "Keywords —maize crops ,disease detection techniques ,syn-\n",
            "thetic images ,augmentation ,CycleGAN, deep learning models ,\n",
            "vision transformers ,accuracy .\n",
            "I. I NTRODUCTION\n",
            "Maize, scientifically known as Zea mays, is of critical\n",
            "importance in agriculture due to its numerous contributions\n",
            "to global food security, economic prosperity, and human\n",
            "well-being. Maize, being one of the world’s most frequently\n",
            "farmed crops, is a major food source for billions of people\n",
            "worldwide, particularly in Africa, Latin America, and Asia.\n",
            "Because of its adaptability, it is used to feed cattle in addition\n",
            "to humans, making it an important part of animal husbandry\n",
            "and the livestock business. In terms of annual production,\n",
            "maize is consistently one of the world’s highest-yieldingcrops. According to the Food and Agriculture Organization\n",
            "(FAO), global maize production has just surpassed 1.1\n",
            "billion metric tons, making it one of the primary contributors\n",
            "to global grain production [1]. This massive production\n",
            "reflects the crop’s tolerance to a wide range of climates\n",
            "and growing circumstances, making it a staple crop in both\n",
            "industrialized and developing countries. Notably, regions\n",
            "such as the United States, China, Brazil, and Argentina\n",
            "stand out as big maize producers, accounting for a sizable\n",
            "proportion of overall annual output.\n",
            "The worldwide maize production environment is not with-\n",
            "out difficulties, with one of the most significant setbacks\n",
            "being the impact of diseases on crop output and quality.\n",
            "Maize plants are susceptible to a variety of diseases, which\n",
            "can severely limit output levels, resulting in large economic\n",
            "losses for farmers and a negative impact on food security.\n",
            "Fungi, bacteria, and viruses, among others, contribute to\n",
            "these losses by creating diseases that impede plant growth,\n",
            "development, and overall production. The fungal disease\n",
            "”Maize Lethal Necrosis” (MLN), for example, has received\n",
            "attention due to its destructive impact on maize output. The\n",
            "virus that causes MLN is a mix of two viruses: Maize\n",
            "chlorotic mottle virus and Sugarcane mosaic virus. This\n",
            "disease spreads quickly through infected planting materials\n",
            "and insect vectors, causing severe stunting, leaf yellowing,\n",
            "and necrosis. According to studies conducted by the In-\n",
            "ternational Maize and Wheat Improvement Center (CIM-\n",
            "MYT), MLN can cause yield losses ranging from 30%\n",
            "to 100%, hurting maize production in East and Central\n",
            "Africa severely [2]. Another well-known disease is ”Maize\n",
            "Rust,” which is caused by the fungus Puccinia sorghi. This\n",
            "disease damages maize leaves, causing rust-colored lesions\n",
            "that limit photosynthesis and impede nutrient absorption.\n",
            "979-8-3503-7029-4/23/$31.00 ©2023 IEEEMaize rust outbreaks have caused output losses of up to 50%\n",
            "in regions such as Asia, Africa, and the Americas, according\n",
            "to researchers [3].\n",
            "Historically, plant disease detection methods relied on\n",
            "visual inspection by experienced professionals to identify\n",
            "signs and symptoms of plant diseases. This procedure entails\n",
            "manually inspecting plant leaves, stems, fruits, and other\n",
            "parts for distinctive patterns, discolorations, lesions, defor-\n",
            "mities, and other signs of disease presence. This procedure,\n",
            "however, is time-consuming, labor-intensive, and frequently\n",
            "prone to human mistakes. Furthermore, the knowledge re-\n",
            "quired to correctly identify a wide range of plant diseases\n",
            "may be confined to experienced specialists. The landscape\n",
            "of plant disease detection has changed due to technological\n",
            "breakthroughs and the rise of machine learning. Machine\n",
            "learning approaches have transformed the accuracy, speed,\n",
            "and scalability of disease detection systems. These methods\n",
            "make use of algorithms and computational models to assess\n",
            "enormous quantities of plant images and other pertinent data,\n",
            "allowing them to generalize and diagnose diseases across\n",
            "plant species and growth phases.\n",
            "GANs (Generative Adversarial Networks) have emerged\n",
            "as an advanced technology with impressive potential in\n",
            "disease identification. GANs have been used to improve\n",
            "disease detection systems in a variety of fields, including\n",
            "the medical and agricultural sectors, in recent years. The\n",
            "application of Vision Transformers (ViTs) in plant disease\n",
            "detection has resulted in significant advances in computer\n",
            "vision and agricultural technology. ViTs are a sort of deep\n",
            "learning architecture that was originally intended for image\n",
            "classification tasks, and their application to plant disease\n",
            "detection demonstrates their adaptability and usefulness in\n",
            "handling complicated real-world problems. Convolutional\n",
            "Neural Networks (CNNs) have traditionally been the go-\n",
            "to solution for image-based tasks such as illness diagnosis.\n",
            "ViTs, on the other hand, have emerged as a promising\n",
            "alternative that challenges the traditional paradigm. Unlike\n",
            "CNNs, which use convolutional layers to extract local\n",
            "features, ViTs take a different approach, transforming 2D\n",
            "picture data into sequences of flattened patches that are\n",
            "subsequently processed using a succession of self-attention\n",
            "methods.\n",
            "This study’s novelty is as follows:\n",
            "1) This study explores the application of GAN, in par-\n",
            "ticular CycleGAN, to address the challenges posed\n",
            "by a limited dataset in maize leaf disease analysis\n",
            "and it includes synthetic image generation and dataset\n",
            "augmentation.\n",
            "2) The evaluation phase encompasses the utilization of\n",
            "CNN and ViT models to assess performance. A com-\n",
            "parison of datasets with and without CycleGAN aug-\n",
            "mentation is conducted in this analysis. The objective\n",
            "of this method is to pinpoint superior models and to\n",
            "measure the influence of GAN’s performance.\n",
            "3) The assessment’s thoroughness is demonstrated by the\n",
            "wide range of metrics it takes into account, including\n",
            "accuracy, loss, precision, recall, and f1-score. This all-encompassing strategy guarantees a thorough compre-\n",
            "hension and understanding of the significance served\n",
            "by GAN and model performance.\n",
            "II. L ITERATURE REVIEW\n",
            "Recent advancements in the field of maize leaf disease\n",
            "detection have witnessed the integration of machine learn-\n",
            "ing techniques, particularly Convolutional Neural Networks\n",
            "(CNNs), to enhance accuracy and efficiency. Ramar Ahila\n",
            "Priyadharshini et al. [4] presented a deep convolutional\n",
            "neural network (CNN)-based architecture for maize leaf dis-\n",
            "ease classification (modified LeNet). The suggested CNNs\n",
            "were trained using maize leaf pictures from the PlantVil-\n",
            "lage dataset, with the goal of identifying four separate\n",
            "classes, three illnesses and one healthy class. In diagnosing\n",
            "maize leaf diseases, the learned model had a remarkable\n",
            "accuracy of 97.89%. Abdul Waheed et al. [5]developed\n",
            "a dense convolutional neural network (CNN) architecture\n",
            "(DenseNet) for detecting and classifying corn leaf diseases.\n",
            "The study addresses the problem of recognizing early-\n",
            "stage leaf diseases in maize crops, with the goal of im-\n",
            "proving crop health monitoring and output quality. The\n",
            "suggested optimized DenseNet model outperformed existing\n",
            "CNN designs such as EfficientNet, VGG19Net, NASNet,\n",
            "and Xception Net while requiring fewer parameters and\n",
            "computation time. Kshyanaprava Panda Panigrahi et al. [6]\n",
            "introduced a CNN-based method for detecting maize leaf\n",
            "diseases, emphasizing the importance of precise and early\n",
            "disease detection in enabling digital agricultural systems.\n",
            "To improve detection accuracy, the suggested CNN model\n",
            "included corrected linear unit activation functions, an Adam\n",
            "optimizer, and parameter tweaks. The model demonstrated\n",
            "quicker training convergence durations while achieving an\n",
            "exceptional average detection accuracy of 98.78% for three\n",
            "major maize leaf diseases, including Cercospora leaf spot,\n",
            "common rust, and northern leaf blight. IHAI ZHANG et\n",
            "al. [7] presented an improved method for automatically\n",
            "detecting maize leaf diseases using deep convolutional\n",
            "neural networks. The study offered improved GoogLeNet\n",
            "and Cifar10 models for leaf disease recognition, with an\n",
            "emphasis on parameter modifications, pooling combinations,\n",
            "dropout operations, and the use of rectified linear unit\n",
            "(Relu) functions. For recognizing eight forms of maize leaf\n",
            "diseases, the GoogLeNet model scored an amazing top-1\n",
            "average identification accuracy of 98.9%, while the Cifar10\n",
            "model achieved an average accuracy of 98.8%.\n",
            "Sumita Mishra et al. [8] used a deep convolutional neural\n",
            "network (CNN) to construct a real-time maize plant disease\n",
            "recognition system. The project aimed to reduce crop losses\n",
            "caused by corn diseases in India by employing smart devices\n",
            "for autonomous disease detection. The deep CNN model\n",
            "was adjusted by adjusting hyperparameters and pooling\n",
            "combinations. The model recognized maize leaf illnesses\n",
            "with an accuracy of 88.46%, making it suitable for real-time\n",
            "inference. Mohammad Syarief et al. [9] used convolutional\n",
            "neural networks (CNNs) to classify images of maize leaf\n",
            "disease. They used 200 photos and divided them into fourcategories: healthy, cercospora, common rust, and northern\n",
            "leaf blight. Seven CNN models (AlexNet, VGG16, VGG19,\n",
            "ResNet50, ResNet101, GoogleNet, and Inception-V3) and\n",
            "machine learning algorithms (SVM, kNN, Decision Tree)\n",
            "were used for feature extraction and categorization. AlexNet\n",
            "with SVM produced the best classification results, with an\n",
            "accuracy of 93.5%, sensitivity of 95.08%, and specificity\n",
            "of 93%. Dionis A. Padilla et al. [10] used a Convolutional\n",
            "Neural Network (CNN) with OpenMP implementation to\n",
            "detect corn leaf illnesses. The study used CNN and OpenMP\n",
            "to identify and classify illnesses in corn leaves. The study’s\n",
            "accuracy in detecting Leaf Blight, Leaf Rust, and Leaf Spot\n",
            "was 93%, 89%, and 89%, respectively. Yan Zhang et al.\n",
            "[11] suggested a method for detecting maize leaf disease\n",
            "using a Convolutional Neural Network (CNN) optimized\n",
            "by a Multi-Activation Function (MAF) module. The goal\n",
            "of the study was to improve the accuracy of traditional\n",
            "AI algorithms for disease identification. Due to the limited\n",
            "dataset, image pre-processing procedures were applied to\n",
            "extend and supplement the disease samples. To expedite\n",
            "training, transfer learning and warm-up procedures were\n",
            "used. The suggested MAF module enhanced the accuracy of\n",
            "mainstream CNNs, with MAF-ResNet50 achieving 97.41%\n",
            "accuracy.\n",
            "In recent years, the field of plant disease detection\n",
            "has witnessed innovative approaches that leverage Vision\n",
            "Transformers (ViTs) and Generative Adversarial Networks\n",
            "(GANs) for enhanced accuracy and robustness in disease\n",
            "identification [12]. Yan Zhang et al. [13] suggested a method\n",
            "for detecting maize leaf disease using a Convolutional Neu-\n",
            "ral Network (CNN) optimized by a Multi-Activation Func-\n",
            "tion (MAF) module. The goal of the study was to improve\n",
            "the accuracy of traditional AI algorithms for disease identi-\n",
            "fication. Due to the limited dataset, image pre-processing\n",
            "procedures were applied to extend and supplement the\n",
            "illness samples. To expedite training, transfer learning, and\n",
            "warm-up procedures were used. The suggested MAF module\n",
            "enhanced the accuracy of mainstream CNNs, with MAF-\n",
            "ResNet50 achieving 97.41% accuracy. Yasamin Borhani et\n",
            "al. [14] proposed using Vision Transformer (ViT) a deep\n",
            "learning approach for automated plant disease classification.\n",
            "Regarding the classification of maize leaf diseases using\n",
            "CNN-based models, Vision Transformer (ViT), and Cycle-\n",
            "GAN for data augmentation, we conducted a review of\n",
            "the literature and identified several gaps and research lim-\n",
            "itations. A brief overview of these challenges is provided\n",
            "below:\n",
            "1) Limited investigation of GAN approaches, notably Cy-\n",
            "cleGAN, as efficient data augmentation methods for\n",
            "overcoming the difficulties of maize leaf disease anal-\n",
            "ysis.\n",
            "2) There is a paucity of studies looking at the poten-\n",
            "tial of Vision Transformer (ViT) and Convolutional\n",
            "Neural Network (CNN) based models working with\n",
            "CycleGAN-augmented datasets to identify maize leaf\n",
            "diseases.III. M ETHODOLOGY\n",
            "The proposed methodology for detecting maize leaf dis-\n",
            "ease is based on a systematic approach that integrates\n",
            "several methodologies to improve accuracy. It all starts with\n",
            "gathering a dataset of maize leaf images [15]. This dataset\n",
            "is then pre-processed to ensure consistency and quality. The\n",
            "methodology relies on CycleGAN, a generative model, to\n",
            "alter and generate synthetic images. This synthetic image\n",
            "production step takes advantage of CycleGAN’s strengths\n",
            "to generate enhanced data with disease characteristics, in-\n",
            "creasing the dataset’s diversity and size.\n",
            "The resulting expanded dataset is then used to train\n",
            "both Convolutional Neural Network (CNN) models and\n",
            "Vision Transformers (ViTs). CNN-based models excel at\n",
            "detecting localized features and patterns, but ViTs detect\n",
            "global context inside images. The maize leaf disease de-\n",
            "tection model achieves complete image analysis, allowing\n",
            "accurate identification of diverse diseases by combining the\n",
            "capabilities of both CNNs and ViTs. In order to identify\n",
            "the optimal model, assessment metrics including accuracy,\n",
            "loss, precision, recall, and f1-score were examined both with\n",
            "and without the Cyclegan augmentation dataset. The overall\n",
            "procedure has been demonstrated in Fig. 1.\n",
            "Data Pre-\n",
            "processingCycleGANSynthetic Image\n",
            "Generation\n",
            "Augmented\n",
            "DatasetCNN and V iT\n",
            "ModelsMaize Leaf\n",
            "Disease DetectionBlast\n",
            "Common RustCollect Dataset\n",
            "Fig. 1. Overall Workflow of The Study.\n",
            "A. CycleGAN-Enhanced Synthetic Image Generation\n",
            "CycleGAN, a variant of Generative Adversarial Networks\n",
            "(GANs), plays a crucial part in enhancing images by trans-\n",
            "lating one image into another. CycleGAN, in contrast to\n",
            "conventional augmentation methods, permits the creation\n",
            "of artificial pictures in a domain without the necessity for\n",
            "paired training data. Within the CycleGAN architecture, this\n",
            "novel method uses the dynamic interaction of two generators\n",
            "and two discriminators.\n",
            "Generator A ( GA→B) transforms images from domain\n",
            "A to domain B. Given an image xAfrom domain A, the\n",
            "generator aims to produce a corresponding image xA→Bin\n",
            "domain B.\n",
            "xA→B=GA→B(xA)\n",
            "Similarly, Generator B ( GB→A) converts images from do-\n",
            "main B to domain A.\n",
            "xB→A=GB→A(xB)\n",
            "The discriminators, DAandDB, distinguish between real\n",
            "images from their respective domains and generated (fake)images. DAassesses the realism of images from domain A,\n",
            "while DBevaluates images from domain B.\n",
            "DA(xA)measures the realism of xA\n",
            "DB(xB)measures the realism of xB\n",
            "The goal of CycleGAN is to achieve image translation in\n",
            "a way that preserves the underlying content of the images.\n",
            "To accomplish this, it introduces cycle-consistency loss,\n",
            "ensuring that an image translated and then translated back\n",
            "remains similar to the original. Equation 1 is the cycle-\n",
            "consistency loss of the model.\n",
            "Lcyc(GA→B, GB→A) =ExA[∥xA−GB→A(GA→B(xA))∥1]\n",
            "(1)\n",
            "Image A\n",
            "Cyclic AGenerated\n",
            "BCompute\n",
            "Lossupdate\n",
            "updateupdate\n",
            "Image B\n",
            "Cyclic BGenerated\n",
            "ACompute\n",
            "Lossupdate\n",
            "updateupdateGenerator\n",
            "A2B\n",
            "Generator\n",
            "B2ADiscriminator\n",
            "B\n",
            "Discriminator\n",
            "AGenerator\n",
            "B2ADiscriminator\n",
            "BDiscriminator\n",
            "A\n",
            "Fake Real\n",
            "Fake RealFake RealReal Fake\n",
            "Generator\n",
            "A2B\n",
            "Fig. 2. CycleGAN architecture.\n",
            "In this instance, the capability of CycleGAN is utilized\n",
            "to convert images of healthy maize leaves into simulated\n",
            "counterparts showing the signs of blight and common rust\n",
            "maize disease images. The Generator A to B, which learns\n",
            "to change pristine leaf pictures into artificially diseased ones,\n",
            "facilitates this transformation. In contrast, Generator B to A\n",
            "produces pictures of maize leaves that appear to be in good\n",
            "condition from diseased inputs. Discriminator A compares\n",
            "the veracity of produced diseased images to real diseased\n",
            "ones, whereas Discriminator B examines the veracity of\n",
            "produced images of healthy leaves. The architecture of\n",
            "CycleGAN is illustrated in Fig. 2.\n",
            "B. Vision Transformers (ViT) model\n",
            "With the use of self-attention mechanisms and transformer\n",
            "architectures, the concept of Vision Transformers (ViT) has\n",
            "become a paradigm shift in image classification. ViT adopts\n",
            "a patch-based methodology in place of conventional Con-\n",
            "volutional Neural Networks (CNNs), turning pictures into\n",
            "sequences of patches. Each patch is hierarchically encoded\n",
            "and attentively interconnected, enabling the model to graspintricate contextual relationships and patterns across the\n",
            "image. The ”Transformer Encoder” design, which enables\n",
            "the integration of self-attention techniques across patches,\n",
            "is a critical invention of ViT. This allows the model to\n",
            "incorporate global context information, which is required for\n",
            "recognizing complex patterns and relationships in images.\n",
            "Attention (Q, K, V ) =softmax\u0012QKT\n",
            "√dk\u0013\n",
            "V (2)\n",
            "where Q,K, and Vare query, key, and value matrices,\n",
            "respectively, and dkis the dimension of the key matrix.\n",
            "ViT-B/16 improves classification accuracy by including\n",
            "wider patches, allowing for a stronger contextual knowledge\n",
            "that is necessary for differentiating between cases of blight\n",
            "and common rust maize leaf. The ”/16” stands for the patch\n",
            "size that was used to separate the input image into smaller,\n",
            "more manageable portions for processing.\n",
            "ViT-B/32 enhances classification performance even further\n",
            "by enlarging patches, allowing for greater resolution analysis\n",
            "and enhancing the difference between blight and common\n",
            "rust categories in maize leaf images. The ”/32” indicates the\n",
            "size of the provided image patch.\n",
            "Fig. 3 depicts the Vision Transformer (ViT) model’s\n",
            "architecture. ViT can capture long-range dependencies and\n",
            "contextual subtleties naturally due to this feature, which is\n",
            "essential for precise image classification.\n",
            "Linear Projection of Flattened PatchesTransformer Encoder\n",
            "1 2 8 9 7 6 5 4 3 0*MLP\n",
            "HeadClass\n",
            "Blight\n",
            "Common Rust+\n",
            "+\n",
            "Embedded\n",
            "PatchesNormMulti-Head\n",
            "AttentionNormMLPTransformer Encoder\n",
            "L ×\n",
            "Patch + Position\n",
            "Embedding\n",
            "* Extra learnable\n",
            "[class] embedding\n",
            "Fig. 3. Vision Transformers (ViT) architecture.\n",
            "C. CNN-based Model\n",
            "Convolutional neural networks, also known as CNNs,\n",
            "which architectures collect and interpret features using con-\n",
            "volutional layers, pooling layers, and fully connected layers,\n",
            "which enables them to handle complex visual patterns. For\n",
            "the purpose of detecting maize leaf disease in this study,\n",
            "a variety of deep-learning models were used. These mod-\n",
            "els include VGG16, ResNet50V2, Xception, MobileNetV2,\n",
            "DenseNet169, and VGG19. These models each provide\n",
            "distinctive architectural characteristics that enhance their\n",
            "performance in picture categorization tasks.\n",
            "DenseNet169 is known for its dense connection patterns,\n",
            "where each layer is linked to all succeeding layers, en-\n",
            "abling rich feature reuse and boosting gradient flow. On\n",
            "CycleGAN-Based Data Augmentation with CNN\n",
            "and Vision Transformers (ViT) Models for\n",
            "Improved Maize Leaf Disease Classification\n",
            "Syed Taha Yeasin Ramadan\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "tahayeasin11@gmail.comTanjim Sakib\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "tsakib77@gmail.com\n",
            "Md. Ahsan Rahat\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "ahsanrahat11@gmail.comShakil Mosharrof\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "shakilmrf8@gmail.com\n",
            "Abstract —Crop losses pose a serious danger to global food\n",
            "security, and this problem also affects maize crops. To success-\n",
            "fully address this issue, precise disease detection techniques\n",
            "are required. However, a major hurdle to developing reliable\n",
            "models to address this issue is the dearth of datasets. In re-\n",
            "sponse, we present a novel approach that uses synthetic images\n",
            "created by CycleGAN to supplement constrained datasets. We\n",
            "thoroughly assessed deep learning models, such as ResNet50V2,\n",
            "DenseNet169, VGG16, VGG19, Xception, MobileNetV2, and\n",
            "emerging vision transformer models, such as ViT-B/16 and\n",
            "ViT-B/32, with a focus on the two critical classes of maize leaf\n",
            "diseases, blight and common rust. Notably, DenseNet169 per-\n",
            "formed better than other models with an accuracy of 98.48%,\n",
            "especially when trained on the CycleGAN-enhanced dataset.\n",
            "CycleGAN-augmented data outperformed the performance of\n",
            "the models trained solely on the original dataset, demonstrating\n",
            "the effectiveness of the augmentation approach in performance\n",
            "enhancement. By utilizing CycleGAN’s synthetic images, this\n",
            "study expands the field of maize leaf disease diagnosis and\n",
            "establishes DenseNet169 as a viable model for precise disease\n",
            "identification. The findings of the study have the potential\n",
            "to significantly revolutionize agricultural operations using ad-\n",
            "vanced maize disease detection techniques.\n",
            "Keywords —maize crops ,disease detection techniques ,syn-\n",
            "thetic images ,augmentation ,CycleGAN, deep learning models ,\n",
            "vision transformers ,accuracy .\n",
            "I. I NTRODUCTION\n",
            "Maize, scientifically known as Zea mays, is of critical\n",
            "importance in agriculture due to its numerous contributions\n",
            "to global food security, economic prosperity, and human\n",
            "well-being. Maize, being one of the world’s most frequently\n",
            "farmed crops, is a major food source for billions of people\n",
            "worldwide, particularly in Africa, Latin America, and Asia.\n",
            "Because of its adaptability, it is used to feed cattle in addition\n",
            "to humans, making it an important part of animal husbandry\n",
            "and the livestock business. In terms of annual production,\n",
            "maize is consistently one of the world’s highest-yieldingcrops. According to the Food and Agriculture Organization\n",
            "(FAO), global maize production has just surpassed 1.1\n",
            "billion metric tons, making it one of the primary contributors\n",
            "to global grain production [1]. This massive production\n",
            "reflects the crop’s tolerance to a wide range of climates\n",
            "and growing circumstances, making it a staple crop in both\n",
            "industrialized and developing countries. Notably, regions\n",
            "such as the United States, China, Brazil, and Argentina\n",
            "stand out as big maize producers, accounting for a sizable\n",
            "proportion of overall annual output.\n",
            "The worldwide maize production environment is not with-\n",
            "out difficulties, with one of the most significant setbacks\n",
            "being the impact of diseases on crop output and quality.\n",
            "Maize plants are susceptible to a variety of diseases, which\n",
            "can severely limit output levels, resulting in large economic\n",
            "losses for farmers and a negative impact on food security.\n",
            "Fungi, bacteria, and viruses, among others, contribute to\n",
            "these losses by creating diseases that impede plant growth,\n",
            "development, and overall production. The fungal disease\n",
            "”Maize Lethal Necrosis” (MLN), for example, has received\n",
            "attention due to its destructive impact on maize output. The\n",
            "virus that causes MLN is a mix of two viruses: Maize\n",
            "chlorotic mottle virus and Sugarcane mosaic virus. This\n",
            "disease spreads quickly through infected planting materials\n",
            "and insect vectors, causing severe stunting, leaf yellowing,\n",
            "and necrosis. According to studies conducted by the In-\n",
            "ternational Maize and Wheat Improvement Center (CIM-\n",
            "MYT), MLN can cause yield losses ranging from 30%\n",
            "to 100%, hurting maize production in East and Central\n",
            "Africa severely [2]. Another well-known disease is ”Maize\n",
            "Rust,” which is caused by the fungus Puccinia sorghi. This\n",
            "disease damages maize leaves, causing rust-colored lesions\n",
            "that limit photosynthesis and impede nutrient absorption.\n",
            "979-8-3503-7029-4/23/$31.00 ©2023 IEEEMaize rust outbreaks have caused output losses of up to 50%\n",
            "in regions such as Asia, Africa, and the Americas, according\n",
            "to researchers [3].\n",
            "Historically, plant disease detection methods relied on\n",
            "visual inspection by experienced professionals to identify\n",
            "signs and symptoms of plant diseases. This procedure entails\n",
            "manually inspecting plant leaves, stems, fruits, and other\n",
            "parts for distinctive patterns, discolorations, lesions, defor-\n",
            "mities, and other signs of disease presence. This procedure,\n",
            "however, is time-consuming, labor-intensive, and frequently\n",
            "prone to human mistakes. Furthermore, the knowledge re-\n",
            "quired to correctly identify a wide range of plant diseases\n",
            "may be confined to experienced specialists. The landscape\n",
            "of plant disease detection has changed due to technological\n",
            "breakthroughs and the rise of machine learning. Machine\n",
            "learning approaches have transformed the accuracy, speed,\n",
            "and scalability of disease detection systems. These methods\n",
            "make use of algorithms and computational models to assess\n",
            "enormous quantities of plant images and other pertinent data,\n",
            "allowing them to generalize and diagnose diseases across\n",
            "plant species and growth phases.\n",
            "GANs (Generative Adversarial Networks) have emerged\n",
            "as an advanced technology with impressive potential in\n",
            "disease identification. GANs have been used to improve\n",
            "disease detection systems in a variety of fields, including\n",
            "the medical and agricultural sectors, in recent years. The\n",
            "application of Vision Transformers (ViTs) in plant disease\n",
            "detection has resulted in significant advances in computer\n",
            "vision and agricultural technology. ViTs are a sort of deep\n",
            "learning architecture that was originally intended for image\n",
            "classification tasks, and their application to plant disease\n",
            "detection demonstrates their adaptability and usefulness in\n",
            "handling complicated real-world problems. Convolutional\n",
            "Neural Networks (CNNs) have traditionally been the go-\n",
            "to solution for image-based tasks such as illness diagnosis.\n",
            "ViTs, on the other hand, have emerged as a promising\n",
            "alternative that challenges the traditional paradigm. Unlike\n",
            "CNNs, which use convolutional layers to extract local\n",
            "features, ViTs take a different approach, transforming 2D\n",
            "picture data into sequences of flattened patches that are\n",
            "subsequently processed using a succession of self-attention\n",
            "methods.\n",
            "This study’s novelty is as follows:\n",
            "1) This study explores the application of GAN, in par-\n",
            "ticular CycleGAN, to address the challenges posed\n",
            "by a limited dataset in maize leaf disease analysis\n",
            "and it includes synthetic image generation and dataset\n",
            "augmentation.\n",
            "2) The evaluation phase encompasses the utilization of\n",
            "CNN and ViT models to assess performance. A com-\n",
            "parison of datasets with and without CycleGAN aug-\n",
            "mentation is conducted in this analysis. The objective\n",
            "of this method is to pinpoint superior models and to\n",
            "measure the influence of GAN’s performance.\n",
            "3) The assessment’s thoroughness is demonstrated by the\n",
            "wide range of metrics it takes into account, including\n",
            "accuracy, loss, precision, recall, and f1-score. This all-encompassing strategy guarantees a thorough compre-\n",
            "hension and understanding of the significance served\n",
            "by GAN and model performance.\n",
            "II. L ITERATURE REVIEW\n",
            "Recent advancements in the field of maize leaf disease\n",
            "detection have witnessed the integration of machine learn-\n",
            "ing techniques, particularly Convolutional Neural Networks\n",
            "(CNNs), to enhance accuracy and efficiency. Ramar Ahila\n",
            "Priyadharshini et al. [4] presented a deep convolutional\n",
            "neural network (CNN)-based architecture for maize leaf dis-\n",
            "ease classification (modified LeNet). The suggested CNNs\n",
            "were trained using maize leaf pictures from the PlantVil-\n",
            "lage dataset, with the goal of identifying four separate\n",
            "classes, three illnesses and one healthy class. In diagnosing\n",
            "maize leaf diseases, the learned model had a remarkable\n",
            "accuracy of 97.89%. Abdul Waheed et al. [5]developed\n",
            "a dense convolutional neural network (CNN) architecture\n",
            "(DenseNet) for detecting and classifying corn leaf diseases.\n",
            "The study addresses the problem of recognizing early-\n",
            "stage leaf diseases in maize crops, with the goal of im-\n",
            "proving crop health monitoring and output quality. The\n",
            "suggested optimized DenseNet model outperformed existing\n",
            "CNN designs such as EfficientNet, VGG19Net, NASNet,\n",
            "and Xception Net while requiring fewer parameters and\n",
            "computation time. Kshyanaprava Panda Panigrahi et al. [6]\n",
            "introduced a CNN-based method for detecting maize leaf\n",
            "diseases, emphasizing the importance of precise and early\n",
            "disease detection in enabling digital agricultural systems.\n",
            "To improve detection accuracy, the suggested CNN model\n",
            "included corrected linear unit activation functions, an Adam\n",
            "optimizer, and parameter tweaks. The model demonstrated\n",
            "quicker training convergence durations while achieving an\n",
            "exceptional average detection accuracy of 98.78% for three\n",
            "major maize leaf diseases, including Cercospora leaf spot,\n",
            "common rust, and northern leaf blight. IHAI ZHANG et\n",
            "al. [7] presented an improved method for automatically\n",
            "detecting maize leaf diseases using deep convolutional\n",
            "neural networks. The study offered improved GoogLeNet\n",
            "and Cifar10 models for leaf disease recognition, with an\n",
            "emphasis on parameter modifications, pooling combinations,\n",
            "dropout operations, and the use of rectified linear unit\n",
            "(Relu) functions. For recognizing eight forms of maize leaf\n",
            "diseases, the GoogLeNet model scored an amazing top-1\n",
            "average identification accuracy of 98.9%, while the Cifar10\n",
            "model achieved an average accuracy of 98.8%.\n",
            "Sumita Mishra et al. [8] used a deep convolutional neural\n",
            "network (CNN) to construct a real-time maize plant disease\n",
            "recognition system. The project aimed to reduce crop losses\n",
            "caused by corn diseases in India by employing smart devices\n",
            "for autonomous disease detection. The deep CNN model\n",
            "was adjusted by adjusting hyperparameters and pooling\n",
            "combinations. The model recognized maize leaf illnesses\n",
            "with an accuracy of 88.46%, making it suitable for real-time\n",
            "inference. Mohammad Syarief et al. [9] used convolutional\n",
            "neural networks (CNNs) to classify images of maize leaf\n",
            "disease. They used 200 photos and divided them into fourcategories: healthy, cercospora, common rust, and northern\n",
            "leaf blight. Seven CNN models (AlexNet, VGG16, VGG19,\n",
            "ResNet50, ResNet101, GoogleNet, and Inception-V3) and\n",
            "machine learning algorithms (SVM, kNN, Decision Tree)\n",
            "were used for feature extraction and categorization. AlexNet\n",
            "with SVM produced the best classification results, with an\n",
            "accuracy of 93.5%, sensitivity of 95.08%, and specificity\n",
            "of 93%. Dionis A. Padilla et al. [10] used a Convolutional\n",
            "Neural Network (CNN) with OpenMP implementation to\n",
            "detect corn leaf illnesses. The study used CNN and OpenMP\n",
            "to identify and classify illnesses in corn leaves. The study’s\n",
            "accuracy in detecting Leaf Blight, Leaf Rust, and Leaf Spot\n",
            "was 93%, 89%, and 89%, respectively. Yan Zhang et al.\n",
            "[11] suggested a method for detecting maize leaf disease\n",
            "using a Convolutional Neural Network (CNN) optimized\n",
            "by a Multi-Activation Function (MAF) module. The goal\n",
            "of the study was to improve the accuracy of traditional\n",
            "AI algorithms for disease identification. Due to the limited\n",
            "dataset, image pre-processing procedures were applied to\n",
            "extend and supplement the disease samples. To expedite\n",
            "training, transfer learning and warm-up procedures were\n",
            "used. The suggested MAF module enhanced the accuracy of\n",
            "mainstream CNNs, with MAF-ResNet50 achieving 97.41%\n",
            "accuracy.\n",
            "In recent years, the field of plant disease detection\n",
            "has witnessed innovative approaches that leverage Vision\n",
            "Transformers (ViTs) and Generative Adversarial Networks\n",
            "(GANs) for enhanced accuracy and robustness in disease\n",
            "identification [12]. Yan Zhang et al. [13] suggested a method\n",
            "for detecting maize leaf disease using a Convolutional Neu-\n",
            "ral Network (CNN) optimized by a Multi-Activation Func-\n",
            "tion (MAF) module. The goal of the study was to improve\n",
            "the accuracy of traditional AI algorithms for disease identi-\n",
            "fication. Due to the limited dataset, image pre-processing\n",
            "procedures were applied to extend and supplement the\n",
            "illness samples. To expedite training, transfer learning, and\n",
            "warm-up procedures were used. The suggested MAF module\n",
            "enhanced the accuracy of mainstream CNNs, with MAF-\n",
            "ResNet50 achieving 97.41% accuracy. Yasamin Borhani et\n",
            "al. [14] proposed using Vision Transformer (ViT) a deep\n",
            "learning approach for automated plant disease classification.\n",
            "Regarding the classification of maize leaf diseases using\n",
            "CNN-based models, Vision Transformer (ViT), and Cycle-\n",
            "GAN for data augmentation, we conducted a review of\n",
            "the literature and identified several gaps and research lim-\n",
            "itations. A brief overview of these challenges is provided\n",
            "below:\n",
            "1) Limited investigation of GAN approaches, notably Cy-\n",
            "cleGAN, as efficient data augmentation methods for\n",
            "overcoming the difficulties of maize leaf disease anal-\n",
            "ysis.\n",
            "2) There is a paucity of studies looking at the poten-\n",
            "tial of Vision Transformer (ViT) and Convolutional\n",
            "Neural Network (CNN) based models working with\n",
            "CycleGAN-augmented datasets to identify maize leaf\n",
            "diseases.III. M ETHODOLOGY\n",
            "The proposed methodology for detecting maize leaf dis-\n",
            "ease is based on a systematic approach that integrates\n",
            "several methodologies to improve accuracy. It all starts with\n",
            "gathering a dataset of maize leaf images [15]. This dataset\n",
            "is then pre-processed to ensure consistency and quality. The\n",
            "methodology relies on CycleGAN, a generative model, to\n",
            "alter and generate synthetic images. This synthetic image\n",
            "production step takes advantage of CycleGAN’s strengths\n",
            "to generate enhanced data with disease characteristics, in-\n",
            "creasing the dataset’s diversity and size.\n",
            "The resulting expanded dataset is then used to train\n",
            "both Convolutional Neural Network (CNN) models and\n",
            "Vision Transformers (ViTs). CNN-based models excel at\n",
            "detecting localized features and patterns, but ViTs detect\n",
            "global context inside images. The maize leaf disease de-\n",
            "tection model achieves complete image analysis, allowing\n",
            "accurate identification of diverse diseases by combining the\n",
            "capabilities of both CNNs and ViTs. In order to identify\n",
            "the optimal model, assessment metrics including accuracy,\n",
            "loss, precision, recall, and f1-score were examined both with\n",
            "and without the Cyclegan augmentation dataset. The overall\n",
            "procedure has been demonstrated in Fig. 1.\n",
            "Data Pre-\n",
            "processingCycleGANSynthetic Image\n",
            "Generation\n",
            "Augmented\n",
            "DatasetCNN and V iT\n",
            "ModelsMaize Leaf\n",
            "Disease DetectionBlast\n",
            "Common RustCollect Dataset\n",
            "Fig. 1. Overall Workflow of The Study.\n",
            "A. CycleGAN-Enhanced Synthetic Image Generation\n",
            "CycleGAN, a variant of Generative Adversarial Networks\n",
            "(GANs), plays a crucial part in enhancing images by trans-\n",
            "lating one image into another. CycleGAN, in contrast to\n",
            "conventional augmentation methods, permits the creation\n",
            "of artificial pictures in a domain without the necessity for\n",
            "paired training data. Within the CycleGAN architecture, this\n",
            "novel method uses the dynamic interaction of two generators\n",
            "and two discriminators.\n",
            "Generator A ( GA→B) transforms images from domain\n",
            "A to domain B. Given an image xAfrom domain A, the\n",
            "generator aims to produce a corresponding image xA→Bin\n",
            "domain B.\n",
            "xA→B=GA→B(xA)\n",
            "Similarly, Generator B ( GB→A) converts images from do-\n",
            "main B to domain A.\n",
            "xB→A=GB→A(xB)\n",
            "The discriminators, DAandDB, distinguish between real\n",
            "images from their respective domains and generated (fake)images. DAassesses the realism of images from domain A,\n",
            "while DBevaluates images from domain B.\n",
            "DA(xA)measures the realism of xA\n",
            "DB(xB)measures the realism of xB\n",
            "The goal of CycleGAN is to achieve image translation in\n",
            "a way that preserves the underlying content of the images.\n",
            "To accomplish this, it introduces cycle-consistency loss,\n",
            "ensuring that an image translated and then translated back\n",
            "remains similar to the original. Equation 1 is the cycle-\n",
            "consistency loss of the model.\n",
            "Lcyc(GA→B, GB→A) =ExA[∥xA−GB→A(GA→B(xA))∥1]\n",
            "(1)\n",
            "Image A\n",
            "Cyclic AGenerated\n",
            "BCompute\n",
            "Lossupdate\n",
            "updateupdate\n",
            "Image B\n",
            "Cyclic BGenerated\n",
            "ACompute\n",
            "Lossupdate\n",
            "updateupdateGenerator\n",
            "A2B\n",
            "Generator\n",
            "B2ADiscriminator\n",
            "B\n",
            "Discriminator\n",
            "AGenerator\n",
            "B2ADiscriminator\n",
            "BDiscriminator\n",
            "A\n",
            "Fake Real\n",
            "Fake RealFake RealReal Fake\n",
            "Generator\n",
            "A2B\n",
            "Fig. 2. CycleGAN architecture.\n",
            "In this instance, the capability of CycleGAN is utilized\n",
            "to convert images of healthy maize leaves into simulated\n",
            "counterparts showing the signs of blight and common rust\n",
            "maize disease images. The Generator A to B, which learns\n",
            "to change pristine leaf pictures into artificially diseased ones,\n",
            "facilitates this transformation. In contrast, Generator B to A\n",
            "produces pictures of maize leaves that appear to be in good\n",
            "condition from diseased inputs. Discriminator A compares\n",
            "the veracity of produced diseased images to real diseased\n",
            "ones, whereas Discriminator B examines the veracity of\n",
            "produced images of healthy leaves. The architecture of\n",
            "CycleGAN is illustrated in Fig. 2.\n",
            "B. Vision Transformers (ViT) model\n",
            "With the use of self-attention mechanisms and transformer\n",
            "architectures, the concept of Vision Transformers (ViT) has\n",
            "become a paradigm shift in image classification. ViT adopts\n",
            "a patch-based methodology in place of conventional Con-\n",
            "volutional Neural Networks (CNNs), turning pictures into\n",
            "sequences of patches. Each patch is hierarchically encoded\n",
            "and attentively interconnected, enabling the model to graspintricate contextual relationships and patterns across the\n",
            "image. The ”Transformer Encoder” design, which enables\n",
            "the integration of self-attention techniques across patches,\n",
            "is a critical invention of ViT. This allows the model to\n",
            "incorporate global context information, which is required for\n",
            "recognizing complex patterns and relationships in images.\n",
            "Attention (Q, K, V ) =softmax\u0012QKT\n",
            "√dk\u0013\n",
            "V (2)\n",
            "where Q,K, and Vare query, key, and value matrices,\n",
            "respectively, and dkis the dimension of the key matrix.\n",
            "ViT-B/16 improves classification accuracy by including\n",
            "wider patches, allowing for a stronger contextual knowledge\n",
            "that is necessary for differentiating between cases of blight\n",
            "and common rust maize leaf. The ”/16” stands for the patch\n",
            "size that was used to separate the input image into smaller,\n",
            "more manageable portions for processing.\n",
            "ViT-B/32 enhances classification performance even further\n",
            "by enlarging patches, allowing for greater resolution analysis\n",
            "and enhancing the difference between blight and common\n",
            "rust categories in maize leaf images. The ”/32” indicates the\n",
            "size of the provided image patch.\n",
            "Fig. 3 depicts the Vision Transformer (ViT) model’s\n",
            "architecture. ViT can capture long-range dependencies and\n",
            "contextual subtleties naturally due to this feature, which is\n",
            "essential for precise image classification.\n",
            "Linear Projection of Flattened PatchesTransformer Encoder\n",
            "1 2 8 9 7 6 5 4 3 0*MLP\n",
            "HeadClass\n",
            "Blight\n",
            "Common Rust+\n",
            "+\n",
            "Embedded\n",
            "PatchesNormMulti-Head\n",
            "AttentionNormMLPTransformer Encoder\n",
            "L ×\n",
            "Patch + Position\n",
            "Embedding\n",
            "* Extra learnable\n",
            "[class] embedding\n",
            "Fig. 3. Vision Transformers (ViT) architecture.\n",
            "C. CNN-based Model\n",
            "Convolutional neural networks, also known as CNNs,\n",
            "which architectures collect and interpret features using con-\n",
            "volutional layers, pooling layers, and fully connected layers,\n",
            "which enables them to handle complex visual patterns. For\n",
            "the purpose of detecting maize leaf disease in this study,\n",
            "a variety of deep-learning models were used. These mod-\n",
            "els include VGG16, ResNet50V2, Xception, MobileNetV2,\n",
            "DenseNet169, and VGG19. These models each provide\n",
            "distinctive architectural characteristics that enhance their\n",
            "performance in picture categorization tasks.\n",
            "DenseNet169 is known for its dense connection patterns,\n",
            "where each layer is linked to all succeeding layers, en-\n",
            "abling rich feature reuse and boosting gradient flow. Onthe other hand, VGG16 is a frequently used architecture\n",
            "in the area since it consists of a series of convolutional\n",
            "and pooling layers, comprising 16 layers. VGG19 adheres\n",
            "to the same architectural principles as VGG16 but has 19\n",
            "layers. ResNet50V2 uses residual connections to address\n",
            "the vanishing gradient issue and makes it possible to train\n",
            "tremendously deep networks. Xception’s architecture aims\n",
            "to capture fine-grained features through a hierarchy of trans-\n",
            "formations. MobileNetV2, which stresses efficiency through\n",
            "lightweight separable convolutions, is targeted for mobile\n",
            "and embedded devices.\n",
            "The Table I provides key factors for CNN-based and ViT\n",
            "models training, with 50 epochs and a learning rate of 0.001\n",
            "determining training duration and weight updates. By high-\n",
            "lighting robust optimization and a multi-class classification\n",
            "focus, the AdamW optimizer and Categorical Cross-entropy\n",
            "loss function improve transparency and reproducibility.\n",
            "TABLE I\n",
            "HYPERPARAMETERS FOR MODELS\n",
            "Epochs Learning Rate Optimizer Loss Function\n",
            "50 0.001 AdamW Categorical Crossentropy\n",
            "IV. E XPERIMENTS AND RESULTS\n",
            "The maize leaf disease dataset originally had 1306 sam-\n",
            "ples for common rust and 1146 samples for blight. Original\n",
            "dataset samples have been shown in Fig. 4. The dataset was\n",
            "considerably enriched through CycleGAN augmentation,\n",
            "generating 1154 blight samples and 994 common rust sam-\n",
            "ples. The dataset was adequately balanced as a consequence,\n",
            "yielding 2300 samples in total for each class. The state of\n",
            "the dataset after augmentation has been delineated in Table\n",
            "II.\n",
            "Blight\n",
            "Common\n",
            "Rust\n",
            "Fig. 4. Maize disease samples.\n",
            "Fig. 5 demonstrated the blight and common rust dis-\n",
            "ease classes’ representative samples, were produced using\n",
            "CycleGAN. These illustrations demonstrate CycleGAN’s\n",
            "capability to provide diverse and realistic disease-related\n",
            "variations.\n",
            "Blight\n",
            "Common\n",
            "RustCycleGAN Generated Synthetic Samples\n",
            "Fig. 5. CycleGAN generated synthetic samples.Table III and Fig. 7 showcase the results obtained from\n",
            "various models in the context of detecting maize leaf dis-\n",
            "eases. The evaluation is based on multiple metrics, including\n",
            "accuracy, loss, precision, recall, and F1-score.\n",
            "In the context of augmentation, the CycleGAN method\n",
            "was employed to augment the dataset, leading to improved\n",
            "model performance. Notably, the ViT-B/16 model achieved\n",
            "an accuracy of 98.37% with a loss of 0.1146, while the\n",
            "ViT-B/32 and VGG16 models attained accuracies of 97.93%\n",
            "with loss values of 0.0970 and 0.1366, respectively. The\n",
            "CycleGAN augmented ResNet50V2 and Xception models\n",
            "demonstrated accuracies of 97.61% and 97.50%, respec-\n",
            "tively, indicating their robustness in disease detection. The\n",
            "MobileNetV2 model achieved an impressive accuracy of\n",
            "98.37% with a notably low loss of 0.0684. However, the\n",
            "DenseNet169 model outperformed others with an accuracy\n",
            "of 98.48% and a significantly low loss of 0.0652, along with\n",
            "high precision, recall, and F1-score values.\n",
            "On the other hand, the table also presents results without\n",
            "augmentation. In this scenario, the models exhibited slightly\n",
            "lower accuracy values. Among these, the MobileNetV2\n",
            "model achieved an accuracy of 97.96% with a loss of 0.0798,\n",
            "while the DenseNet169 model demonstrated an accuracy\n",
            "of 97.76% and a loss of 0.0902. These results emphasize\n",
            "the importance of dataset augmentation, particularly through\n",
            "techniques like CycleGAN, in enhancing the performance of\n",
            "maize leaf disease detection models.\n",
            "TABLE II\n",
            "AUGMENTED DATASET\n",
            "Disease Initial CycleGAN Total\n",
            "Samples Augmented Samples Cumulative Samples\n",
            "Blight 1146 1154 2300\n",
            "Common Rust 1306 994 2300\n",
            "Total 2452 2148 4600\n",
            "TABLE III\n",
            "RESULT OBTAINED BYVARIOUS MODELS\n",
            "Augmentation Model Accuracy Loss Precision Recall F1-score\n",
            "Method (%)\n",
            "ViT-B/16 98.37 0.1146 0.9837 0.9837 0.9837\n",
            "ViT-B/32 97.93 0.0970 0.9794 0.9793 0.9793\n",
            "VGG16 97.93 0.1366 0.9793 0.9793 0.9793\n",
            "CycleGAN ResNet50V2 97.61 0.1574 0.9762 0.9761 0.9761\n",
            "Xception 97.50 0.1291 0.9754 0.975 0.975\n",
            "MobileNetV2 98.37 0.0684 0.9837 0.9837 0.9837\n",
            "DenseNet169 98.48 0.0652 0.9848 0.9848 0.9848\n",
            "VGG19 97.72 0.1228 0.9772 0.9772 0.9772\n",
            "ViT-B/16 96.95 0.1695 0.9695 0.9695 0.9695\n",
            "ViT-B/32 96.13 0.2120 0.9613 0.9613 0.9613\n",
            "VGG16 95.93 0.2187 0.9594 0.9593 0.9593\n",
            "Without ResNet50V2 94.91 0.2626 0.9498 0.9491 0.9491\n",
            "Augmentation Xception 95.72 0.2950 0.9572 0.9572 0.9572\n",
            "MobileNetV2 97.96 0.0798 0.9799 0.9796 0.9796\n",
            "DenseNet169 97.76 0.0902 0.9778 0.9776 0.9776\n",
            "VGG19 96.54 0.1805 0.9654 0.9654 0.9654\n",
            "CycleGAN-Based Data Augmentation with CNN\n",
            "and Vision Transformers (ViT) Models for\n",
            "Improved Maize Leaf Disease Classification\n",
            "Syed Taha Yeasin Ramadan\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "tahayeasin11@gmail.comTanjim Sakib\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "tsakib77@gmail.com\n",
            "Md. Ahsan Rahat\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "ahsanrahat11@gmail.comShakil Mosharrof\n",
            "Department of Computer Science and Engineering\n",
            "Military Institute of Science and Technology (MIST)\n",
            "Dhaka, Bangladesh\n",
            "shakilmrf8@gmail.com\n",
            "Abstract —Crop losses pose a serious danger to global food\n",
            "security, and this problem also affects maize crops. To success-\n",
            "fully address this issue, precise disease detection techniques\n",
            "are required. However, a major hurdle to developing reliable\n",
            "models to address this issue is the dearth of datasets. In re-\n",
            "sponse, we present a novel approach that uses synthetic images\n",
            "created by CycleGAN to supplement constrained datasets. We\n",
            "thoroughly assessed deep learning models, such as ResNet50V2,\n",
            "DenseNet169, VGG16, VGG19, Xception, MobileNetV2, and\n",
            "emerging vision transformer models, such as ViT-B/16 and\n",
            "ViT-B/32, with a focus on the two critical classes of maize leaf\n",
            "diseases, blight and common rust. Notably, DenseNet169 per-\n",
            "formed better than other models with an accuracy of 98.48%,\n",
            "especially when trained on the CycleGAN-enhanced dataset.\n",
            "CycleGAN-augmented data outperformed the performance of\n",
            "the models trained solely on the original dataset, demonstrating\n",
            "the effectiveness of the augmentation approach in performance\n",
            "enhancement. By utilizing CycleGAN’s synthetic images, this\n",
            "study expands the field of maize leaf disease diagnosis and\n",
            "establishes DenseNet169 as a viable model for precise disease\n",
            "identification. The findings of the study have the potential\n",
            "to significantly revolutionize agricultural operations using ad-\n",
            "vanced maize disease detection techniques.\n",
            "Keywords —maize crops ,disease detection techniques ,syn-\n",
            "thetic images ,augmentation ,CycleGAN, deep learning models ,\n",
            "vision transformers ,accuracy .\n",
            "I. I NTRODUCTION\n",
            "Maize, scientifically known as Zea mays, is of critical\n",
            "importance in agriculture due to its numerous contributions\n",
            "to global food security, economic prosperity, and human\n",
            "well-being. Maize, being one of the world’s most frequently\n",
            "farmed crops, is a major food source for billions of people\n",
            "worldwide, particularly in Africa, Latin America, and Asia.\n",
            "Because of its adaptability, it is used to feed cattle in addition\n",
            "to humans, making it an important part of animal husbandry\n",
            "and the livestock business. In terms of annual production,\n",
            "maize is consistently one of the world’s highest-yieldingcrops. According to the Food and Agriculture Organization\n",
            "(FAO), global maize production has just surpassed 1.1\n",
            "billion metric tons, making it one of the primary contributors\n",
            "to global grain production [1]. This massive production\n",
            "reflects the crop’s tolerance to a wide range of climates\n",
            "and growing circumstances, making it a staple crop in both\n",
            "industrialized and developing countries. Notably, regions\n",
            "such as the United States, China, Brazil, and Argentina\n",
            "stand out as big maize producers, accounting for a sizable\n",
            "proportion of overall annual output.\n",
            "The worldwide maize production environment is not with-\n",
            "out difficulties, with one of the most significant setbacks\n",
            "being the impact of diseases on crop output and quality.\n",
            "Maize plants are susceptible to a variety of diseases, which\n",
            "can severely limit output levels, resulting in large economic\n",
            "losses for farmers and a negative impact on food security.\n",
            "Fungi, bacteria, and viruses, among others, contribute to\n",
            "these losses by creating diseases that impede plant growth,\n",
            "development, and overall production. The fungal disease\n",
            "”Maize Lethal Necrosis” (MLN), for example, has received\n",
            "attention due to its destructive impact on maize output. The\n",
            "virus that causes MLN is a mix of two viruses: Maize\n",
            "chlorotic mottle virus and Sugarcane mosaic virus. This\n",
            "disease spreads quickly through infected planting materials\n",
            "and insect vectors, causing severe stunting, leaf yellowing,\n",
            "and necrosis. According to studies conducted by the In-\n",
            "ternational Maize and Wheat Improvement Center (CIM-\n",
            "MYT), MLN can cause yield losses ranging from 30%\n",
            "to 100%, hurting maize production in East and Central\n",
            "Africa severely [2]. Another well-known disease is ”Maize\n",
            "Rust,” which is caused by the fungus Puccinia sorghi. This\n",
            "disease damages maize leaves, causing rust-colored lesions\n",
            "that limit photosynthesis and impede nutrient absorption.\n",
            "979-8-3503-7029-4/23/$31.00 ©2023 IEEEMaize rust outbreaks have caused output losses of up to 50%\n",
            "in regions such as Asia, Africa, and the Americas, according\n",
            "to researchers [3].\n",
            "Historically, plant disease detection methods relied on\n",
            "visual inspection by experienced professionals to identify\n",
            "signs and symptoms of plant diseases. This procedure entails\n",
            "manually inspecting plant leaves, stems, fruits, and other\n",
            "parts for distinctive patterns, discolorations, lesions, defor-\n",
            "mities, and other signs of disease presence. This procedure,\n",
            "however, is time-consuming, labor-intensive, and frequently\n",
            "prone to human mistakes. Furthermore, the knowledge re-\n",
            "quired to correctly identify a wide range of plant diseases\n",
            "may be confined to experienced specialists. The landscape\n",
            "of plant disease detection has changed due to technological\n",
            "breakthroughs and the rise of machine learning. Machine\n",
            "learning approaches have transformed the accuracy, speed,\n",
            "and scalability of disease detection systems. These methods\n",
            "make use of algorithms and computational models to assess\n",
            "enormous quantities of plant images and other pertinent data,\n",
            "allowing them to generalize and diagnose diseases across\n",
            "plant species and growth phases.\n",
            "GANs (Generative Adversarial Networks) have emerged\n",
            "as an advanced technology with impressive potential in\n",
            "disease identification. GANs have been used to improve\n",
            "disease detection systems in a variety of fields, including\n",
            "the medical and agricultural sectors, in recent years. The\n",
            "application of Vision Transformers (ViTs) in plant disease\n",
            "detection has resulted in significant advances in computer\n",
            "vision and agricultural technology. ViTs are a sort of deep\n",
            "learning architecture that was originally intended for image\n",
            "classification tasks, and their application to plant disease\n",
            "detection demonstrates their adaptability and usefulness in\n",
            "handling complicated real-world problems. Convolutional\n",
            "Neural Networks (CNNs) have traditionally been the go-\n",
            "to solution for image-based tasks such as illness diagnosis.\n",
            "ViTs, on the other hand, have emerged as a promising\n",
            "alternative that challenges the traditional paradigm. Unlike\n",
            "CNNs, which use convolutional layers to extract local\n",
            "features, ViTs take a different approach, transforming 2D\n",
            "picture data into sequences of flattened patches that are\n",
            "subsequently processed using a succession of self-attention\n",
            "methods.\n",
            "This study’s novelty is as follows:\n",
            "1) This study explores the application of GAN, in par-\n",
            "ticular CycleGAN, to address the challenges posed\n",
            "by a limited dataset in maize leaf disease analysis\n",
            "and it includes synthetic image generation and dataset\n",
            "augmentation.\n",
            "2) The evaluation phase encompasses the utilization of\n",
            "CNN and ViT models to assess performance. A com-\n",
            "parison of datasets with and without CycleGAN aug-\n",
            "mentation is conducted in this analysis. The objective\n",
            "of this method is to pinpoint superior models and to\n",
            "measure the influence of GAN’s performance.\n",
            "3) The assessment’s thoroughness is demonstrated by the\n",
            "wide range of metrics it takes into account, including\n",
            "accuracy, loss, precision, recall, and f1-score. This all-encompassing strategy guarantees a thorough compre-\n",
            "hension and understanding of the significance served\n",
            "by GAN and model performance.\n",
            "II. L ITERATURE REVIEW\n",
            "Recent advancements in the field of maize leaf disease\n",
            "detection have witnessed the integration of machine learn-\n",
            "ing techniques, particularly Convolutional Neural Networks\n",
            "(CNNs), to enhance accuracy and efficiency. Ramar Ahila\n",
            "Priyadharshini et al. [4] presented a deep convolutional\n",
            "neural network (CNN)-based architecture for maize leaf dis-\n",
            "ease classification (modified LeNet). The suggested CNNs\n",
            "were trained using maize leaf pictures from the PlantVil-\n",
            "lage dataset, with the goal of identifying four separate\n",
            "classes, three illnesses and one healthy class. In diagnosing\n",
            "maize leaf diseases, the learned model had a remarkable\n",
            "accuracy of 97.89%. Abdul Waheed et al. [5]developed\n",
            "a dense convolutional neural network (CNN) architecture\n",
            "(DenseNet) for detecting and classifying corn leaf diseases.\n",
            "The study addresses the problem of recognizing early-\n",
            "stage leaf diseases in maize crops, with the goal of im-\n",
            "proving crop health monitoring and output quality. The\n",
            "suggested optimized DenseNet model outperformed existing\n",
            "CNN designs such as EfficientNet, VGG19Net, NASNet,\n",
            "and Xception Net while requiring fewer parameters and\n",
            "computation time. Kshyanaprava Panda Panigrahi et al. [6]\n",
            "introduced a CNN-based method for detecting maize leaf\n",
            "diseases, emphasizing the importance of precise and early\n",
            "disease detection in enabling digital agricultural systems.\n",
            "To improve detection accuracy, the suggested CNN model\n",
            "included corrected linear unit activation functions, an Adam\n",
            "optimizer, and parameter tweaks. The model demonstrated\n",
            "quicker training convergence durations while achieving an\n",
            "exceptional average detection accuracy of 98.78% for three\n",
            "major maize leaf diseases, including Cercospora leaf spot,\n",
            "common rust, and northern leaf blight. IHAI ZHANG et\n",
            "al. [7] presented an improved method for automatically\n",
            "detecting maize leaf diseases using deep convolutional\n",
            "neural networks. The study offered improved GoogLeNet\n",
            "and Cifar10 models for leaf disease recognition, with an\n",
            "emphasis on parameter modifications, pooling combinations,\n",
            "dropout operations, and the use of rectified linear unit\n",
            "(Relu) functions. For recognizing eight forms of maize leaf\n",
            "diseases, the GoogLeNet model scored an amazing top-1\n",
            "average identification accuracy of 98.9%, while the Cifar10\n",
            "model achieved an average accuracy of 98.8%.\n",
            "Sumita Mishra et al. [8] used a deep convolutional neural\n",
            "network (CNN) to construct a real-time maize plant disease\n",
            "recognition system. The project aimed to reduce crop losses\n",
            "caused by corn diseases in India by employing smart devices\n",
            "for autonomous disease detection. The deep CNN model\n",
            "was adjusted by adjusting hyperparameters and pooling\n",
            "combinations. The model recognized maize leaf illnesses\n",
            "with an accuracy of 88.46%, making it suitable for real-time\n",
            "inference. Mohammad Syarief et al. [9] used convolutional\n",
            "neural networks (CNNs) to classify images of maize leaf\n",
            "disease. They used 200 photos and divided them into fourcategories: healthy, cercospora, common rust, and northern\n",
            "leaf blight. Seven CNN models (AlexNet, VGG16, VGG19,\n",
            "ResNet50, ResNet101, GoogleNet, and Inception-V3) and\n",
            "machine learning algorithms (SVM, kNN, Decision Tree)\n",
            "were used for feature extraction and categorization. AlexNet\n",
            "with SVM produced the best classification results, with an\n",
            "accuracy of 93.5%, sensitivity of 95.08%, and specificity\n",
            "of 93%. Dionis A. Padilla et al. [10] used a Convolutional\n",
            "Neural Network (CNN) with OpenMP implementation to\n",
            "detect corn leaf illnesses. The study used CNN and OpenMP\n",
            "to identify and classify illnesses in corn leaves. The study’s\n",
            "accuracy in detecting Leaf Blight, Leaf Rust, and Leaf Spot\n",
            "was 93%, 89%, and 89%, respectively. Yan Zhang et al.\n",
            "[11] suggested a method for detecting maize leaf disease\n",
            "using a Convolutional Neural Network (CNN) optimized\n",
            "by a Multi-Activation Function (MAF) module. The goal\n",
            "of the study was to improve the accuracy of traditional\n",
            "AI algorithms for disease identification. Due to the limited\n",
            "dataset, image pre-processing procedures were applied to\n",
            "extend and supplement the disease samples. To expedite\n",
            "training, transfer learning and warm-up procedures were\n",
            "used. The suggested MAF module enhanced the accuracy of\n",
            "mainstream CNNs, with MAF-ResNet50 achieving 97.41%\n",
            "accuracy.\n",
            "In recent years, the field of plant disease detection\n",
            "has witnessed innovative approaches that leverage Vision\n",
            "Transformers (ViTs) and Generative Adversarial Networks\n",
            "(GANs) for enhanced accuracy and robustness in disease\n",
            "identification [12]. Yan Zhang et al. [13] suggested a method\n",
            "for detecting maize leaf disease using a Convolutional Neu-\n",
            "ral Network (CNN) optimized by a Multi-Activation Func-\n",
            "tion (MAF) module. The goal of the study was to improve\n",
            "the accuracy of traditional AI algorithms for disease identi-\n",
            "fication. Due to the limited dataset, image pre-processing\n",
            "procedures were applied to extend and supplement the\n",
            "illness samples. To expedite training, transfer learning, and\n",
            "warm-up procedures were used. The suggested MAF module\n",
            "enhanced the accuracy of mainstream CNNs, with MAF-\n",
            "ResNet50 achieving 97.41% accuracy. Yasamin Borhani et\n",
            "al. [14] proposed using Vision Transformer (ViT) a deep\n",
            "learning approach for automated plant disease classification.\n",
            "Regarding the classification of maize leaf diseases using\n",
            "CNN-based models, Vision Transformer (ViT), and Cycle-\n",
            "GAN for data augmentation, we conducted a review of\n",
            "the literature and identified several gaps and research lim-\n",
            "itations. A brief overview of these challenges is provided\n",
            "below:\n",
            "1) Limited investigation of GAN approaches, notably Cy-\n",
            "cleGAN, as efficient data augmentation methods for\n",
            "overcoming the difficulties of maize leaf disease anal-\n",
            "ysis.\n",
            "2) There is a paucity of studies looking at the poten-\n",
            "tial of Vision Transformer (ViT) and Convolutional\n",
            "Neural Network (CNN) based models working with\n",
            "CycleGAN-augmented datasets to identify maize leaf\n",
            "diseases.III. M ETHODOLOGY\n",
            "The proposed methodology for detecting maize leaf dis-\n",
            "ease is based on a systematic approach that integrates\n",
            "several methodologies to improve accuracy. It all starts with\n",
            "gathering a dataset of maize leaf images [15]. This dataset\n",
            "is then pre-processed to ensure consistency and quality. The\n",
            "methodology relies on CycleGAN, a generative model, to\n",
            "alter and generate synthetic images. This synthetic image\n",
            "production step takes advantage of CycleGAN’s strengths\n",
            "to generate enhanced data with disease characteristics, in-\n",
            "creasing the dataset’s diversity and size.\n",
            "The resulting expanded dataset is then used to train\n",
            "both Convolutional Neural Network (CNN) models and\n",
            "Vision Transformers (ViTs). CNN-based models excel at\n",
            "detecting localized features and patterns, but ViTs detect\n",
            "global context inside images. The maize leaf disease de-\n",
            "tection model achieves complete image analysis, allowing\n",
            "accurate identification of diverse diseases by combining the\n",
            "capabilities of both CNNs and ViTs. In order to identify\n",
            "the optimal model, assessment metrics including accuracy,\n",
            "loss, precision, recall, and f1-score were examined both with\n",
            "and without the Cyclegan augmentation dataset. The overall\n",
            "procedure has been demonstrated in Fig. 1.\n",
            "Data Pre-\n",
            "processingCycleGANSynthetic Image\n",
            "Generation\n",
            "Augmented\n",
            "DatasetCNN and V iT\n",
            "ModelsMaize Leaf\n",
            "Disease DetectionBlast\n",
            "Common RustCollect Dataset\n",
            "Fig. 1. Overall Workflow of The Study.\n",
            "A. CycleGAN-Enhanced Synthetic Image Generation\n",
            "CycleGAN, a variant of Generative Adversarial Networks\n",
            "(GANs), plays a crucial part in enhancing images by trans-\n",
            "lating one image into another. CycleGAN, in contrast to\n",
            "conventional augmentation methods, permits the creation\n",
            "of artificial pictures in a domain without the necessity for\n",
            "paired training data. Within the CycleGAN architecture, this\n",
            "novel method uses the dynamic interaction of two generators\n",
            "and two discriminators.\n",
            "Generator A ( GA→B) transforms images from domain\n",
            "A to domain B. Given an image xAfrom domain A, the\n",
            "generator aims to produce a corresponding image xA→Bin\n",
            "domain B.\n",
            "xA→B=GA→B(xA)\n",
            "Similarly, Generator B ( GB→A) converts images from do-\n",
            "main B to domain A.\n",
            "xB→A=GB→A(xB)\n",
            "The discriminators, DAandDB, distinguish between real\n",
            "images from their respective domains and generated (fake)images. DAassesses the realism of images from domain A,\n",
            "while DBevaluates images from domain B.\n",
            "DA(xA)measures the realism of xA\n",
            "DB(xB)measures the realism of xB\n",
            "The goal of CycleGAN is to achieve image translation in\n",
            "a way that preserves the underlying content of the images.\n",
            "To accomplish this, it introduces cycle-consistency loss,\n",
            "ensuring that an image translated and then translated back\n",
            "remains similar to the original. Equation 1 is the cycle-\n",
            "consistency loss of the model.\n",
            "Lcyc(GA→B, GB→A) =ExA[∥xA−GB→A(GA→B(xA))∥1]\n",
            "(1)\n",
            "Image A\n",
            "Cyclic AGenerated\n",
            "BCompute\n",
            "Lossupdate\n",
            "updateupdate\n",
            "Image B\n",
            "Cyclic BGenerated\n",
            "ACompute\n",
            "Lossupdate\n",
            "updateupdateGenerator\n",
            "A2B\n",
            "Generator\n",
            "B2ADiscriminator\n",
            "B\n",
            "Discriminator\n",
            "AGenerator\n",
            "B2ADiscriminator\n",
            "BDiscriminator\n",
            "A\n",
            "Fake Real\n",
            "Fake RealFake RealReal Fake\n",
            "Generator\n",
            "A2B\n",
            "Fig. 2. CycleGAN architecture.\n",
            "In this instance, the capability of CycleGAN is utilized\n",
            "to convert images of healthy maize leaves into simulated\n",
            "counterparts showing the signs of blight and common rust\n",
            "maize disease images. The Generator A to B, which learns\n",
            "to change pristine leaf pictures into artificially diseased ones,\n",
            "facilitates this transformation. In contrast, Generator B to A\n",
            "produces pictures of maize leaves that appear to be in good\n",
            "condition from diseased inputs. Discriminator A compares\n",
            "the veracity of produced diseased images to real diseased\n",
            "ones, whereas Discriminator B examines the veracity of\n",
            "produced images of healthy leaves. The architecture of\n",
            "CycleGAN is illustrated in Fig. 2.\n",
            "B. Vision Transformers (ViT) model\n",
            "With the use of self-attention mechanisms and transformer\n",
            "architectures, the concept of Vision Transformers (ViT) has\n",
            "become a paradigm shift in image classification. ViT adopts\n",
            "a patch-based methodology in place of conventional Con-\n",
            "volutional Neural Networks (CNNs), turning pictures into\n",
            "sequences of patches. Each patch is hierarchically encoded\n",
            "and attentively interconnected, enabling the model to graspintricate contextual relationships and patterns across the\n",
            "image. The ”Transformer Encoder” design, which enables\n",
            "the integration of self-attention techniques across patches,\n",
            "is a critical invention of ViT. This allows the model to\n",
            "incorporate global context information, which is required for\n",
            "recognizing complex patterns and relationships in images.\n",
            "Attention (Q, K, V ) =softmax\u0012QKT\n",
            "√dk\u0013\n",
            "V (2)\n",
            "where Q,K, and Vare query, key, and value matrices,\n",
            "respectively, and dkis the dimension of the key matrix.\n",
            "ViT-B/16 improves classification accuracy by including\n",
            "wider patches, allowing for a stronger contextual knowledge\n",
            "that is necessary for differentiating between cases of blight\n",
            "and common rust maize leaf. The ”/16” stands for the patch\n",
            "size that was used to separate the input image into smaller,\n",
            "more manageable portions for processing.\n",
            "ViT-B/32 enhances classification performance even further\n",
            "by enlarging patches, allowing for greater resolution analysis\n",
            "and enhancing the difference between blight and common\n",
            "rust categories in maize leaf images. The ”/32” indicates the\n",
            "size of the provided image patch.\n",
            "Fig. 3 depicts the Vision Transformer (ViT) model’s\n",
            "architecture. ViT can capture long-range dependencies and\n",
            "contextual subtleties naturally due to this feature, which is\n",
            "essential for precise image classification.\n",
            "Linear Projection of Flattened PatchesTransformer Encoder\n",
            "1 2 8 9 7 6 5 4 3 0*MLP\n",
            "HeadClass\n",
            "Blight\n",
            "Common Rust+\n",
            "+\n",
            "Embedded\n",
            "PatchesNormMulti-Head\n",
            "AttentionNormMLPTransformer Encoder\n",
            "L ×\n",
            "Patch + Position\n",
            "Embedding\n",
            "* Extra learnable\n",
            "[class] embedding\n",
            "Fig. 3. Vision Transformers (ViT) architecture.\n",
            "C. CNN-based Model\n",
            "Convolutional neural networks, also known as CNNs,\n",
            "which architectures collect and interpret features using con-\n",
            "volutional layers, pooling layers, and fully connected layers,\n",
            "which enables them to handle complex visual patterns. For\n",
            "the purpose of detecting maize leaf disease in this study,\n",
            "a variety of deep-learning models were used. These mod-\n",
            "els include VGG16, ResNet50V2, Xception, MobileNetV2,\n",
            "DenseNet169, and VGG19. These models each provide\n",
            "distinctive architectural characteristics that enhance their\n",
            "performance in picture categorization tasks.\n",
            "DenseNet169 is known for its dense connection patterns,\n",
            "where each layer is linked to all succeeding layers, en-\n",
            "abling rich feature reuse and boosting gradient flow. Onthe other hand, VGG16 is a frequently used architecture\n",
            "in the area since it consists of a series of convolutional\n",
            "and pooling layers, comprising 16 layers. VGG19 adheres\n",
            "to the same architectural principles as VGG16 but has 19\n",
            "layers. ResNet50V2 uses residual connections to address\n",
            "the vanishing gradient issue and makes it possible to train\n",
            "tremendously deep networks. Xception’s architecture aims\n",
            "to capture fine-grained features through a hierarchy of trans-\n",
            "formations. MobileNetV2, which stresses efficiency through\n",
            "lightweight separable convolutions, is targeted for mobile\n",
            "and embedded devices.\n",
            "The Table I provides key factors for CNN-based and ViT\n",
            "models training, with 50 epochs and a learning rate of 0.001\n",
            "determining training duration and weight updates. By high-\n",
            "lighting robust optimization and a multi-class classification\n",
            "focus, the AdamW optimizer and Categorical Cross-entropy\n",
            "loss function improve transparency and reproducibility.\n",
            "TABLE I\n",
            "HYPERPARAMETERS FOR MODELS\n",
            "Epochs Learning Rate Optimizer Loss Function\n",
            "50 0.001 AdamW Categorical Crossentropy\n",
            "IV. E XPERIMENTS AND RESULTS\n",
            "The maize leaf disease dataset originally had 1306 sam-\n",
            "ples for common rust and 1146 samples for blight. Original\n",
            "dataset samples have been shown in Fig. 4. The dataset was\n",
            "considerably enriched through CycleGAN augmentation,\n",
            "generating 1154 blight samples and 994 common rust sam-\n",
            "ples. The dataset was adequately balanced as a consequence,\n",
            "yielding 2300 samples in total for each class. The state of\n",
            "the dataset after augmentation has been delineated in Table\n",
            "II.\n",
            "Blight\n",
            "Common\n",
            "Rust\n",
            "Fig. 4. Maize disease samples.\n",
            "Fig. 5 demonstrated the blight and common rust dis-\n",
            "ease classes’ representative samples, were produced using\n",
            "CycleGAN. These illustrations demonstrate CycleGAN’s\n",
            "capability to provide diverse and realistic disease-related\n",
            "variations.\n",
            "Blight\n",
            "Common\n",
            "RustCycleGAN Generated Synthetic Samples\n",
            "Fig. 5. CycleGAN generated synthetic samples.Table III and Fig. 7 showcase the results obtained from\n",
            "various models in the context of detecting maize leaf dis-\n",
            "eases. The evaluation is based on multiple metrics, including\n",
            "accuracy, loss, precision, recall, and F1-score.\n",
            "In the context of augmentation, the CycleGAN method\n",
            "was employed to augment the dataset, leading to improved\n",
            "model performance. Notably, the ViT-B/16 model achieved\n",
            "an accuracy of 98.37% with a loss of 0.1146, while the\n",
            "ViT-B/32 and VGG16 models attained accuracies of 97.93%\n",
            "with loss values of 0.0970 and 0.1366, respectively. The\n",
            "CycleGAN augmented ResNet50V2 and Xception models\n",
            "demonstrated accuracies of 97.61% and 97.50%, respec-\n",
            "tively, indicating their robustness in disease detection. The\n",
            "MobileNetV2 model achieved an impressive accuracy of\n",
            "98.37% with a notably low loss of 0.0684. However, the\n",
            "DenseNet169 model outperformed others with an accuracy\n",
            "of 98.48% and a significantly low loss of 0.0652, along with\n",
            "high precision, recall, and F1-score values.\n",
            "On the other hand, the table also presents results without\n",
            "augmentation. In this scenario, the models exhibited slightly\n",
            "lower accuracy values. Among these, the MobileNetV2\n",
            "model achieved an accuracy of 97.96% with a loss of 0.0798,\n",
            "while the DenseNet169 model demonstrated an accuracy\n",
            "of 97.76% and a loss of 0.0902. These results emphasize\n",
            "the importance of dataset augmentation, particularly through\n",
            "techniques like CycleGAN, in enhancing the performance of\n",
            "maize leaf disease detection models.\n",
            "TABLE II\n",
            "AUGMENTED DATASET\n",
            "Disease Initial CycleGAN Total\n",
            "Samples Augmented Samples Cumulative Samples\n",
            "Blight 1146 1154 2300\n",
            "Common Rust 1306 994 2300\n",
            "Total 2452 2148 4600\n",
            "TABLE III\n",
            "RESULT OBTAINED BYVARIOUS MODELS\n",
            "Augmentation Model Accuracy Loss Precision Recall F1-score\n",
            "Method (%)\n",
            "ViT-B/16 98.37 0.1146 0.9837 0.9837 0.9837\n",
            "ViT-B/32 97.93 0.0970 0.9794 0.9793 0.9793\n",
            "VGG16 97.93 0.1366 0.9793 0.9793 0.9793\n",
            "CycleGAN ResNet50V2 97.61 0.1574 0.9762 0.9761 0.9761\n",
            "Xception 97.50 0.1291 0.9754 0.975 0.975\n",
            "MobileNetV2 98.37 0.0684 0.9837 0.9837 0.9837\n",
            "DenseNet169 98.48 0.0652 0.9848 0.9848 0.9848\n",
            "VGG19 97.72 0.1228 0.9772 0.9772 0.9772\n",
            "ViT-B/16 96.95 0.1695 0.9695 0.9695 0.9695\n",
            "ViT-B/32 96.13 0.2120 0.9613 0.9613 0.9613\n",
            "VGG16 95.93 0.2187 0.9594 0.9593 0.9593\n",
            "Without ResNet50V2 94.91 0.2626 0.9498 0.9491 0.9491\n",
            "Augmentation Xception 95.72 0.2950 0.9572 0.9572 0.9572\n",
            "MobileNetV2 97.96 0.0798 0.9799 0.9796 0.9796\n",
            "DenseNet169 97.76 0.0902 0.9778 0.9776 0.9776\n",
            "VGG19 96.54 0.1805 0.9654 0.9654 0.9654Fig. 6. Accuracy and confusion matrix of DenseNet169 on CycleGAN-\n",
            "augmented dataset.\n",
            "0.930.940.950.960.970.980.99\n",
            "ViT-B/16\n",
            "ViT-B/32\n",
            "VGG16\n",
            "ResNet50V2\n",
            "Xception\n",
            "MobileNetV2\n",
            "DenseNet169\n",
            "VGG19\n",
            "ViT-B/16ViT-B/32\n",
            "VGG16\n",
            "ResNet50V2\n",
            "Xception\n",
            "MobileNetV2\n",
            "DenseNet169\n",
            "VGG19\n",
            "CycleGAN No Augmentation\n",
            "Precision Recall F1-score\n",
            "Fig. 7. Comparative analysis of evaluation metrics using CycleGAN\n",
            "augmented and non-augmented datasets for diverse models.\n",
            "The accuracy and confusion matrix of the best-performing\n",
            "model, DenseNet169, are illustrated in Fig. 6. The accuracy\n",
            "achieved by DenseNet169 was 98.48%, with a correspond-\n",
            "ing loss of 0.0652. The model demonstrated remarkable\n",
            "precision, recall, and F1-score values of 0.9848, 0.9848, and\n",
            "0.9848, respectively. The confusion matrix further provides\n",
            "insights into the model’s performance for each class, show-\n",
            "ing the true positive and true negative predictions as well as\n",
            "potential areas for improvement.\n",
            "V. C ONCLUSION AND FUTURE WORK\n",
            "This work demonstrated the efficacy of CycleGAN-\n",
            "generated image augmentation in improving the preciseness\n",
            "of maize leaf disease classification using various CNN-\n",
            "based and Vision Transformer (ViT) models. The findings\n",
            "underscore CycleGAN’s potential to overcome the problems\n",
            "presented by limited datasets, leading to more accurate\n",
            "disease identification. Our test results demonstrated that the\n",
            "DenseNet169 outperformed the other models, obtaining a\n",
            "maximum accuracy of 98.48% in the CycleGAN expanded\n",
            "dataset. Furthermore, the study encompasses the integration\n",
            "of a Vision Transformer (ViT) model within the scope of\n",
            "maize leaf disease detection. Future research could focus on\n",
            "adjusting CycleGAN parameters for specific disease traits\n",
            "and investigating hybrid model architectures that include\n",
            "the advantages of CNNs and ViTs for even more reliable\n",
            "classification outcomes. The study might further broaden its\n",
            "focus to include other crop diseases, which would increasethe usefulness of the suggested methodology in the field of\n",
            "managing agricultural diseases.\n",
            "REFERENCES\n",
            "[1] Food and Agriculture Organization (FAO), World Food and\n",
            "Agriculture - Statistical Pocketbook 2021 . Rome, Italy: FAO, 2021.\n",
            "[Online]. Available: http://www.fao.org/3/cb4535en/CB4535EN.pdf\n",
            "[2] I. Macharia, N. Wanyera, and P. S. Ojiambo, “Comparative assessment\n",
            "of maize lethal necrosis disease incidence and severity under varying\n",
            "agroecological conditions in kenya,” Plant Disease , vol. 101, no. 5,\n",
            "pp. 709–716, 2017.\n",
            "[3] R. P. Singh, D. P. Hodson, J. Huerta-Espino, Y . Jin, P. Njau, R. Wany-\n",
            "era, and S. Herrera-Foessel, “Will stem rust destroy the world’s wheat\n",
            "crop?” Advances in Agronomy , vol. 98, pp. 271–309, 2008.\n",
            "[4] R. Ahila Priyadharshini, S. Arivazhagan, M. Arun, and A. Mirnalini,\n",
            "“Maize leaf disease classification using deep convolutional neural\n",
            "networks,” Neural Computing and Applications , vol. 31, pp. 8887–\n",
            "8895, 2019.\n",
            "[5] A. Waheed, M. Goyal, D. Gupta, A. Khanna, A. E. Hassanien, and\n",
            "H. M. Pandey, “An optimized dense convolutional neural network\n",
            "model for disease recognition and classification in corn leaf,” Com-\n",
            "puters and Electronics in Agriculture , vol. 175, p. 105456, 2020.\n",
            "[6] K. P. Panigrahi, A. K. Sahoo, and H. Das, “A cnn approach for\n",
            "corn leaves disease detection to support digital agricultural system,”\n",
            "in2020 4th International Conference on Trends in Electronics and\n",
            "Informatics (ICOEI)(48184) . IEEE, 2020, pp. 678–683.\n",
            "[7] X. Zhang, Y . Qiao, F. Meng, C. Fan, and M. Zhang, “Identification\n",
            "of maize leaf diseases using improved deep convolutional neural\n",
            "networks,” Ieee Access , vol. 6, pp. 30 370–30 377, 2018.\n",
            "[8] S. Mishra, R. Sachan, and D. Rajpal, “Deep convolutional neural\n",
            "network based detection system for real-time corn plant disease\n",
            "recognition,” Procedia Computer Science , vol. 167, pp. 2003–2010,\n",
            "2020.\n",
            "[9] M. Syarief and W. Setiawan, “Convolutional neural network for maize\n",
            "leaf disease image classification,” Telkomnika (Telecommunication\n",
            "Computing Electronics and Control) , vol. 18, no. 3, pp. 1376–1381,\n",
            "2020.\n",
            "[10] D. A. Padilla, R. A. I. Pajes, and J. T. De Guzman, “Detection of\n",
            "corn leaf diseases using convolutional neural network with openmp\n",
            "implementation,” in 2020 IEEE 12th International Conference on\n",
            "Humanoid, Nanotechnology, Information Technology, Communication\n",
            "and Control, Environment, and Management (HNICEM) . IEEE,\n",
            "2020, pp. 1–6.\n",
            "[11] Y . Zhang, S. Wa, Y . Liu, X. Zhou, P. Sun, and Q. Ma, “High-\n",
            "accuracy detection of maize leaf diseases cnn based on multi-pathway\n",
            "activation function module,” Remote Sensing , vol. 13, no. 21, p. 4218,\n",
            "2021.\n",
            "[12] S. T. Y . Ramadan, T. Sakib, M. M. U. Haque, N. Sharmin, and\n",
            "M. M. Rahman, “Generative adversarial network-based augmented\n",
            "rice leaf disease detection using deep learning,” in 2022 25th Interna-\n",
            "tional Conference on Computer and Information Technology (ICCIT) .\n",
            "IEEE, 2022, pp. 976–981.\n",
            "[13] Y . Zhao, Z. Chen, X. Gao, W. Song, Q. Xiong, J. Hu, and Z. Zhang,\n",
            "“Plant disease detection using generated leaves based on doublegan,”\n",
            "IEEE/ACM Transactions on Computational Biology and Bioinformat-\n",
            "ics, vol. 19, no. 3, pp. 1817–1826, 2021.\n",
            "[14] Y . Borhani, J. Khoramdel, and E. Najafi, “A deep learning based\n",
            "approach for automated plant disease classification using vision\n",
            "transformer,” Scientific Reports , vol. 12, no. 1, p. 11554, 2022.\n",
            "[15] S. Ghose, “Corn or maize leaf disease dataset,”\n",
            "https://www.kaggle.com/smaranjitghose/corn-or-maize-leaf-disease-\n",
            "dataset, 2021.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=32)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "FKJmfnpcQVNL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_OcqwInQb8e",
        "outputId": "4625aa65-fb54-42a9-f47b-fdb3f1be9c06"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "MLjC1kAmQi-E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465,
          "referenced_widgets": [
            "7a548280764e451395a4eada51727a88",
            "9c02177090e84eb1a844f008aed891b5",
            "fe1905ac15eb430eaf7523e26fdaf7df",
            "31e7cafb682d48688766cf29690e57eb",
            "d43619363a154cffb858dafd81436ad5",
            "1f50fcf3bd66480c90f5c74688ec62ef",
            "23c3546cdc1c4c91a96abed8000b1c55",
            "45f3c93ef95f4e18b5dd53279d34a259",
            "50c6b35db6fc47569b693cb302e0f538",
            "91101939b3e54cac9aadd6048fd33a88",
            "9ffb50e132944e52ab6fc529dce37ba6",
            "8f20829e982c4a24a364141ae54b3d44",
            "f693db637f6748ed952de43b8bf0b096",
            "0673664975f64a8d9c9a00c76f169f43",
            "7a4e8922948c4b40a9f33c134b4d79a9",
            "daf045668eaa46d1a4e596e4b0775af6",
            "ae26c46039484c0183b01243a7b057fa",
            "bd216d7e420443cca6b6cc188c982c11",
            "42bda4aa40a04e729199434f159943ad",
            "ae90632c262042908e1294bb50e44a8f",
            "42f0ee6fab614316a438ee4436e8e568",
            "d740064bc3354ebfb04bb0cf9bb6e195",
            "7b29a736ab014072a4a62fa4530693b9",
            "15c0995964974ca799a30943450b6833",
            "1e95564a70734dc7af25a5305b4d7482",
            "fe6735d80c7448fd9d31170e2346f61d",
            "3bc3cb6e1a204bb595d008464d658b33",
            "cf322aad77c64a99905045a02bc6e350",
            "c4541a942fc6438ab9e51dfeaff56ba0",
            "30114c6327f34df5a88b478662f898f2",
            "73f967e4718b4c35a46a6da15c2ff9d4",
            "5ec2f7f04752418688efacb3193867ba",
            "5dc9f9c2bd5b4a1393d569930c1fe317",
            "69b82b1a2e0e4e54bf231082844e5a5c",
            "24e8674aa6f94e8cbcd17b1593b8b942",
            "1d5e696f5b4442c89e24756d6d3ecc6f",
            "a4b9b37cfb764255ab5fde6e74446951",
            "a57c456658d340c8ab1a1bc39718b1cd",
            "6722b59bdf92470ca44e83b614e07ced",
            "3db0929544eb4ed4baf1ba74333d97ea",
            "51c45b526d874705bedb7b7de2857db0",
            "ccef1f34b24e4051a38f7e12c6d90cca",
            "74da29651e4d44258fb3ab6687bfae74",
            "c93630edfee14609851115ad97e61397",
            "7e51a823018042a089de8282e651e21c",
            "b39ab82f9107495f994dbb9e8c7e463c",
            "a469e6eedf4b48a3ab8002e2f154c051",
            "91df94c8660d4948825ed08ece80568b",
            "56d5786dd2444f9f92a0f326e79c2b54",
            "98af79139a9a44fa84e3f5398ea8395d",
            "b8c23eedb3fa4782996ce2a1d9b845c4",
            "7f7d6c69e4484bdfa795d319ec72f34f",
            "b4a0d42edc71440cb690b48420935734",
            "73fc05b29f144c4088a284034c0c2b08",
            "f1e07e727d65439191aaca71808f10ff",
            "2a7f9489e6234245a55fafec48d7026a",
            "d280d5ae7fed4d46a1a7735726b15f5b",
            "6ddc37daa89f42fe8d03c47f057ab1c5",
            "3ae530a352c34f318ab135a6956f7568",
            "cc817cd560584ab19ec8cf1331823dc5",
            "8b65e576d6344e488252f287c0c768ee",
            "0ec11022fee1427495d3b043dd6c0f96",
            "c2cd54a1525a4545accd49682f5a12b0",
            "9586dca09fc349948687eca9276e70f9",
            "9b352bc4f7544d2388965888156a4998",
            "9380a122141a4966907afd076fbd1b0e",
            "8bf25ffe79f34aafbdfa2f386576c29b",
            "d3849885469b4d8ca63a5bd84efeb6aa",
            "16e8aabf20e84042a2d6daea8614a319",
            "f6ac549b54eb4d3ab14be4d0b3c53e46",
            "80d76b25ddea463cb4a02baca78b7964",
            "00487600a94f47b5977368caf3d38b3a",
            "20237462196a4f51805652efa64ab1e4",
            "6dba0c3e160a40fbb0345bda384dbb91",
            "b101a93a4a4340869cd959e86aa29edc",
            "b9918ebff3b14159b6e9fb70bbc22fb9",
            "068dd2feea5449a1b0f474c941c10a87",
            "5c46ba72136345bc8c7d8c7968efc66a",
            "6ab36788dcfc4ae0873eb079008f3242",
            "23c3ee7c304b40cf97c923d0fea9bf48",
            "73d8e4f371c942b58eb6515e2f7456b7",
            "51dc8547e46a4b40b5ff8385c8dbd53a",
            "da006f17bcd447dbb25baf2beb17062b",
            "3fba7a9963814f50b531e6033a5a9d3c",
            "18dfd8e4be7b43b38c041401bf07c75b",
            "49e7fb6ced034570ad0535dc6bcfb460",
            "2c6ca3c8e4474628a17dc870a4409284",
            "35ad9ff803c0482da2c74c0d55e9ac87",
            "d0f7ca8252af4b33bfe537f8e8e5635b",
            "558340845f22464eb0c477852e56324b",
            "7a0ce6e4c2fe4df5bb1e9dbc38cd7d03",
            "6815bfc7ac7347fd90914185bea3c7fb",
            "9adbb48984fc4d8fb8cbaf8c148e7c15",
            "fe951f1e6b394186b194c3165ec08144",
            "f3da64cab502401a8e3ffd4601e5472c",
            "55860e7dcb07412f934a4ae73fa0ecfd",
            "dca8f07d4e524dba8490b535749c0917",
            "84c218909a9a4eaa8efbc74064761202",
            "f9f0e8666df54d10963b8d6cd401d441",
            "89273f39f0194b2ba451ac8496fe1f31",
            "df2a6a2cc8cc42ebb7cc30c5fc17fc01",
            "02ec62de6ae1481cb52073f9866fe7c6",
            "1d4eb979a3534dbeb07de1e2878ec419",
            "f3acb2f3915a4614a76769a387e24798",
            "d756dd74486a4326a161501eac495259",
            "dc7205912fdf473ea532d5288ab18221",
            "3a6d7a234e914c3eb6fe46cfa465ade3",
            "4a63800d89d24b15aef336ae4ffb0514",
            "fec1956c67b14862ac434cc5d8e31a62",
            "7f779a66880e4883b34c98ec2e1e079a",
            "b6f91c3f700b48989d78c12aa6692ea0",
            "c5e48d74489344b5a8dea5314b94f8d2",
            "8d435cc50ebb443fa1bb648ebefefdf4",
            "a7b3342a541045e3ae2fe2c9fb8c880e",
            "c7ebcbc82ec34889bd9bd08b0b26ac8f",
            "e4140a1f5a58428a878ac4b72bc8e1d0",
            "4a6027bec64a4402806e6b2981e1e1d7",
            "f96fc837858f450e9b2686de8e735dc4",
            "859a1ed8ab814dde99d42561bca3ad7d",
            "28b775094fff4ecdb9a4fed7e6450880",
            "4ac800db8f0045108c9b57326bb721d0",
            "e6bfd34388b94d8abcd56c9c52234366",
            "eafbb7500ca34f3f9f26412942da3d76",
            "987e0faca77145d2a71558bd9ea5fa3a",
            "775987ff960642578d3a948b6024bfbf",
            "c5c842954a1044bdab7680222ef94911",
            "3d2d9fb3656940799c4101c17bbbe779",
            "c07aef5a98f04d40935cbc2aea99ff02",
            "ed9ef0ae146346b2ae02770ea1274c5b",
            "99b9bdb3589b49c18b66bcf8069acceb",
            "e13b6e0cab2245e2980643b64467aaa5",
            "44ff0738e3164ca3a089015359bed8d5",
            "1cb6f38292e64b31a813b9b407c7d565",
            "6c679f0ca0ad4cc29fae12e18cb73a20",
            "5b9fff8473934ee0b21f0391a44d9ab8",
            "4105ac7f47bf4dcab7725e1e101aa89f",
            "322143288b6345958dc145519ab3c8eb",
            "32af7e0017c348258954bbf2ac970c75",
            "229e224195a84e5d85459a38960ff74e",
            "df766d0b355e4828a7b2d46d8993cf1d",
            "eb183c3c95f24325bda0ac8953d2c379",
            "00222ec14d314f32b60780b25f3c990a",
            "c0c2742de5c74a8e86a4702cc942f512",
            "d6b8bd47e1274528916720a977117d15",
            "dcd2241e61dd4d28a25a8370d2872854",
            "f38253e3d6b240e0beb6d39e0dee907f",
            "d101f15a1e7f4372bb77dd4ecea158ae",
            "b022e02b6211499a9c8e768ea297a2ed",
            "37e3f74996e14330b663885ffbb17c2b",
            "ed818cfc066e44d488a835fbb51d6d6c",
            "53485f79b97640e8b981ffa5f492f6e8",
            "8669c23b9fb14848a39eff02287a127c",
            "24bd38e5af7d49b0ad96d3def5de9238",
            "ccce57a07cc94c36aaae8b4ace864454"
          ]
        },
        "outputId": "d5dde56b-15c3-4870-fcff-2332c9673974"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a548280764e451395a4eada51727a88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f20829e982c4a24a364141ae54b3d44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b29a736ab014072a4a62fa4530693b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69b82b1a2e0e4e54bf231082844e5a5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e51a823018042a089de8282e651e21c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a7f9489e6234245a55fafec48d7026a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bf25ffe79f34aafbdfa2f386576c29b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c46ba72136345bc8c7d8c7968efc66a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0f7ca8252af4b33bfe537f8e8e5635b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89273f39f0194b2ba451ac8496fe1f31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6f91c3f700b48989d78c12aa6692ea0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6bfd34388b94d8abcd56c9c52234366"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1cb6f38292e64b31a813b9b407c7d565"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6b8bd47e1274528916720a977117d15"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FQXuajAXmGD",
        "outputId": "bc2148ea-6458-46b3-baa0-3dff45f8963e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import GPT4All\n",
        "model_n_ctx = 1000\n",
        "model_n_batch=8\n",
        "model_path = \"./gpt4all-falcon-q4_0.gguf\"\n",
        "llm = GPT4All(model=model_path, max_tokens=model_n_ctx, backend='llama', n_batch=model_n_batch, verbose=False)"
      ],
      "metadata": {
        "id": "XDYaPzpeX-Eq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    return_source_documents=True,\n",
        "    verbose=False,\n",
        ")"
      ],
      "metadata": {
        "id": "-cjaqb7yYN2N"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res=qa(\"generate a literature review from this paper. start with : [first-author-name] et al. propose [content]\")\n",
        "print(res)"
      ],
      "metadata": {
        "id": "gfUXXKjtxg-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd3c2af9-df94-439d-8a2b-6d3aa0f52e56"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'generate a literature review from this paper. start with : [first-author-name] et al. propose [content]', 'result': '\\n\\nA literature review is a comprehensive summary of existing research on a particular topic or subject area. In this case, the paper by [first-author-name] et al. proposes the use of CycleGAN for maize leaf disease detection. The authors conducted an extensive evaluation of various models and datasets to determine the most effective approach for detecting maize leaf diseases.\\n\\nThe literature review section of the paper provides a comprehensive overview of existing research on maize leaf diseases, including their symptoms, causes, and management practices. The authors also discuss the limitations of traditional image-based disease detection methods and highlight the need for more accurate and efficient approaches.\\n\\nIn addition to the literature review, the paper presents several key findings and results related to the use of CycleGAN for maize leaf disease detection. These include:\\n\\n* The authors demonstrate that CycleGAN can be used effectively for maize leaf disease detection, achieving high accuracy, recall, and F1-score values.\\n* The authors also show that dataset augmentation, particularly through techniques like CycleGAN, can significantly enhance the performance of maize leaf disease detection models.\\n* The authors compare the performance of various models on the same dataset without and with augmentation, highlighting the importance of dataset augmentation in enhancing model performance.\\n\\nOverall,', 'source_documents': [Document(page_content='high precision, recall, and F1-score values.\\nOn the other hand, the table also presents results without\\naugmentation. In this scenario, the models exhibited slightly\\nlower accuracy values. Among these, the MobileNetV2\\nmodel achieved an accuracy of 97.96% with a loss of 0.0798,\\nwhile the DenseNet169 model demonstrated an accuracy\\nof 97.76% and a loss of 0.0902. These results emphasize\\nthe importance of dataset augmentation, particularly through\\ntechniques like CycleGAN, in enhancing the performance of\\nmaize leaf disease detection models.\\nTABLE II\\nAUGMENTED DATASET\\nDisease Initial CycleGAN Total\\nSamples Augmented Samples Cumulative Samples\\nBlight 1146 1154 2300\\nCommon Rust 1306 994 2300\\nTotal 2452 2148 4600\\nTABLE III\\nRESULT OBTAINED BYVARIOUS MODELS\\nAugmentation Model Accuracy Loss Precision Recall F1-score\\nMethod (%)\\nViT-B/16 98.37 0.1146 0.9837 0.9837 0.9837\\nViT-B/32 97.93 0.0970 0.9794 0.9793 0.9793\\nVGG16 97.93 0.1366 0.9793 0.9793 0.9793\\nCycleGAN ResNet50V2 97.61 0.1574 0.9762 0.9761 0.9761', metadata={'source': '/content/drive/MyDrive/Colab Notebooks/~Pictures/2023279689.pdf', 'page': 4}), Document(page_content='Fig. 6. Accuracy and confusion matrix of DenseNet169 on CycleGAN-\\naugmented dataset.\\n0.930.940.950.960.970.980.99\\nViT-B/16\\nViT-B/32\\nVGG16\\nResNet50V2\\nXception\\nMobileNetV2\\nDenseNet169\\nVGG19\\nViT-B/16ViT-B/32\\nVGG16\\nResNet50V2\\nXception\\nMobileNetV2\\nDenseNet169\\nVGG19\\nCycleGAN No Augmentation\\nPrecision Recall F1-score\\nFig. 7. Comparative analysis of evaluation metrics using CycleGAN\\naugmented and non-augmented datasets for diverse models.\\nThe accuracy and confusion matrix of the best-performing\\nmodel, DenseNet169, are illustrated in Fig. 6. The accuracy\\nachieved by DenseNet169 was 98.48%, with a correspond-\\ning loss of 0.0652. The model demonstrated remarkable\\nprecision, recall, and F1-score values of 0.9848, 0.9848, and\\n0.9848, respectively. The confusion matrix further provides\\ninsights into the model’s performance for each class, show-\\ning the true positive and true negative predictions as well as\\npotential areas for improvement.\\nV. C ONCLUSION AND FUTURE WORK\\nThis work demonstrated the efficacy of CycleGAN-', metadata={'source': '/content/drive/MyDrive/Colab Notebooks/~Pictures/2023279689.pdf', 'page': 5}), Document(page_content='Xception 97.50 0.1291 0.9754 0.975 0.975\\nMobileNetV2 98.37 0.0684 0.9837 0.9837 0.9837', metadata={'source': '/content/drive/MyDrive/Colab Notebooks/~Pictures/2023279689.pdf', 'page': 4})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import GPT4All\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load PDF Data\n",
        "pdf_file = '/content/drive/MyDrive/Colab Notebooks/~Pictures/2023279689.pdf'\n",
        "pdf_reader = PdfReader(open(pdf_file, 'rb'))\n",
        "\n",
        "# Extract text from PDF\n",
        "pdf_text = \"\"\n",
        "for page in pdf_reader.pages:\n",
        "    pdf_text += page.extract_text()\n",
        "\n",
        "# Tokenize the text (example: splitting by sentences)\n",
        "text_tokens = pdf_text.split('. ')  # Splitting by periods to get sentences\n",
        "\n",
        "# Initialize GPT4All Model\n",
        "model_path = \"./gpt4all-falcon-q4_0.gguf\"\n",
        "llm = GPT4All(model=model_path)\n",
        "\n",
        "# Create a Vector Store (Chroma)\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "db = Chroma.from_documents(text_tokens, embeddings, persist_directory=\"db\")\n",
        "\n",
        "# Use RetrievalQA to Generate Literature Review\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    return_source_documents=True,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "# Perform a literature review by providing a query\n",
        "query = \"title\"  # Replace with your query\n",
        "res = qa(query)\n",
        "print(res)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nlk-ysne7Mku"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}